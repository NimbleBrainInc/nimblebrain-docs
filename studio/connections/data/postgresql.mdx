---
title: 'PostgreSQL'
description: 'Industrial-strength database management with AI-powered natural language queries, intelligent index optimization, and comprehensive health monitoring'
---

<CardGroup cols={2}>
  <Card title="Natural Language Database Queries" icon="wand-magic-sparkles">
    Transform plain English questions into optimized SQL queries. Ask about your data naturally and let AI handle the complexity.
  </Card>
  <Card title="Intelligent Index Tuning" icon="gauge-high">
    Automated index recommendations using enterprise-grade algorithms (Microsoft SQL Server DTA). Optimize performance without manual analysis.
  </Card>
</CardGroup>

## Overview

PostgreSQL transforms database management into a natural conversation. Instead of writing complex SQL queries, analyzing execution plans, or manually tuning indexes, you simply describe what you need in plain English. The server intelligently translates your requests into optimized database operations.

**Perfect for:**
- Data analysts who want instant answers without writing SQL
- Developers optimizing application performance
- Database administrators monitoring system health
- Teams wanting to democratize database access safely

**Core capabilities:**
- **Schema Intelligence**: Explore database structure through natural language
- **Safe SQL Execution**: Read-only mode protects production data
- **Query Optimization**: Get execution plans with hypothetical index simulation
- **Performance Tuning**: Industrial-strength index recommendations (DTA algorithm)
- **Health Monitoring**: Comprehensive checks for indexes, connections, vacuum, replication, and more

<Note>
This server supports both managed PostgreSQL services (Supabase, Neon, AWS RDS) and self-hosted instances. All operations are performed through secure database connections.
</Note>

## Quick Start

<Steps>
  <Step title="Set Up PostgreSQL Access">
    Get your database connection details. For managed services like Supabase or Neon, find the connection string in your dashboard. For self-hosted databases, ensure you have the host, port, database name, username, and password.

    <Info>
    **Recommended**: Create a read-only database user for Studio integration to protect against accidental data modifications.
    </Info>
  </Step>

  <Step title="Add Server to NimbleBrain Studio">
    1. Open NimbleBrain Studio
    2. Navigate to the integration Servers page
    3. Search for "PostgreSQL connection"
    4. Click "Add to Workspace"
    5. In the environment variables, add:
       - `DATABASE_URI`: Your full connection string (e.g., `postgresql://user:password@host:port/database`)
    6. For production use, set `--access-mode restricted` to enable read-only safety features
  </Step>

  <Step title="Test Your Connection">
    Open a playbook and try:

    **"Show me all the schemas in my database"**

    The server will list your database schemas. Then try:

    **"What tables are in the public schema?"**

    You should see a list of all tables. Now you're ready to query your database with natural language!
  </Step>
</Steps>

## Available Tools

<AccordionGroup>
  <Accordion title="list_schemas - Discover Database Structure" icon="layer-group">
    Lists all schemas in your PostgreSQL database with their ownership and classification (system vs user schemas).

    **What it does:**
    - Retrieves all schemas from `information_schema.schemata`
    - Classifies schemas as System, System Information, or User schemas
    - Shows schema owners for permission auditing
    - Ordered by type and name for easy navigation

    **Parameters:**
    None - simply lists all schemas

    **Returns:**
    Array of schema objects with:
    - `schema_name`: Name of the schema
    - `schema_owner`: Owner of the schema
    - `schema_type`: Classification (System Schema, System Information Schema, or User Schema)

    **Natural language examples:**
    - "What schemas exist in my database?"
    - "Show me all the database schemas"
    - "List the user-created schemas"
    - "Which schemas are system schemas?"

    **Example response:**
    ```
    [
      {"schema_name": "public", "schema_owner": "postgres", "schema_type": "User Schema"},
      {"schema_name": "auth", "schema_owner": "supabase_admin", "schema_type": "User Schema"},
      {"schema_name": "pg_catalog", "schema_owner": "postgres", "schema_type": "System Schema"}
    ]
    ```

    <Info>
    Use this as your first step when exploring an unfamiliar database. It provides a high-level map of the database structure.
    </Info>
  </Accordion>

  <Accordion title="list_objects - Browse Schema Contents" icon="table">
    Lists database objects within a specific schema, including tables, views, sequences, and extensions.

    **What it does:**
    - Queries `information_schema` tables for object metadata
    - Filters by schema and object type
    - Returns structured information about each object
    - Supports tables, views, sequences, and extensions

    **Parameters:**
    | Parameter | Type | Required | Default | Description |
    |-----------|------|----------|---------|-------------|
    | `schema_name` | string | Yes | - | Schema to query (e.g., "public", "auth") |
    | `object_type` | string | No | "table" | Type of object: "table", "view", "sequence", or "extension" |

    **Returns:**
    Array of objects with properties depending on type:
    - **Tables/Views**: schema, name, type
    - **Sequences**: schema, name, data_type
    - **Extensions**: name, version, relocatable

    **Natural language examples:**
    - "What tables are in the public schema?"
    - "Show me all views in the analytics schema"
    - "List sequences in the public schema"
    - "What extensions are installed?"

    **Example - Tables:**
    ```
    [
      {"schema": "public", "name": "users", "type": "BASE TABLE"},
      {"schema": "public", "name": "orders", "type": "BASE TABLE"},
      {"schema": "public", "name": "products", "type": "BASE TABLE"}
    ]
    ```

    **Example - Extensions:**
    ```
    [
      {"name": "pg_stat_statements", "version": "1.10", "relocatable": false},
      {"name": "hypopg", "version": "1.4.0", "relocatable": true}
    ]
    ```

    <Tip>
    Use `object_type="extension"` to verify that required extensions like `pg_stat_statements` and `hypopg` are installed for advanced features.
    </Tip>
  </Accordion>

  <Accordion title="get_object_details - Inspect Table Structure" icon="magnifying-glass">
    Retrieves detailed information about a specific database object, including columns, constraints, indexes, and more.

    **What it does:**
    - Gets complete structure of tables and views
    - Shows all columns with data types, nullability, and defaults
    - Lists constraints (primary keys, foreign keys, unique, check)
    - Displays all indexes with their definitions
    - Provides sequence and extension details

    **Parameters:**
    | Parameter | Type | Required | Default | Description |
    |-----------|------|----------|---------|-------------|
    | `schema_name` | string | Yes | - | Schema containing the object |
    | `object_name` | string | Yes | - | Name of the object to inspect |
    | `object_type` | string | No | "table" | Type: "table", "view", "sequence", or "extension" |

    **Returns:**
    Detailed object structure with:
    - **Tables/Views**: basic info, columns array, constraints array, indexes array
    - **Sequences**: schema, name, data_type, start_value, increment
    - **Extensions**: name, version, relocatable status

    **Natural language examples:**
    - "Show me the structure of the users table"
    - "What columns does the orders table have?"
    - "What indexes exist on the products table?"
    - "Show me all constraints on the accounts table"

    **Example - Table details:**
    ```
    {
      "basic": {"schema": "public", "name": "users", "type": "table"},
      "columns": [
        {"column": "id", "data_type": "integer", "is_nullable": "NO", "default": "nextval('users_id_seq'::regclass)"},
        {"column": "email", "data_type": "character varying", "is_nullable": "NO", "default": null},
        {"column": "created_at", "data_type": "timestamp", "is_nullable": "YES", "default": "now()"}
      ],
      "constraints": [
        {"name": "users_pkey", "type": "PRIMARY KEY", "columns": ["id"]},
        {"name": "users_email_key", "type": "UNIQUE", "columns": ["email"]}
      ],
      "indexes": [
        {"name": "users_pkey", "definition": "CREATE UNIQUE INDEX users_pkey ON public.users USING btree (id)"},
        {"name": "users_email_idx", "definition": "CREATE INDEX users_email_idx ON public.users USING btree (email)"}
      ]
    }
    ```

    <Info>
    This tool is essential before running complex queries. Understanding table structure helps AI generate more accurate SQL.
    </Info>
  </Accordion>

  <Accordion title="execute_sql - Run Database Queries" icon="play">
    Executes SQL queries against your PostgreSQL database. Supports both unrestricted and read-only (restricted) modes.

    **What it does:**
    - Executes any SQL statement in unrestricted mode
    - Enforces read-only operations in restricted mode
    - Returns query results as structured data
    - Provides timeout protection (30s in restricted mode)
    - Prevents destructive operations in production

    **Parameters:**
    | Parameter | Type | Required | Default | Description |
    |-----------|------|----------|---------|-------------|
    | `sql` | string | Yes | - | SQL query to execute |

    **Access Modes:**
    - **Unrestricted** (Development): Allows all SQL operations including INSERT, UPDATE, DELETE, DROP
    - **Restricted** (Production): Only allows SELECT queries, enforces 30-second timeout, blocks destructive operations

    **Natural language examples:**
    - "Show me the first 10 users"
    - "Count how many orders were placed today"
    - "Find all products with price greater than 100"
    - "Get the average order value by month"

    **Example query:**
    ```sql
    SELECT id, email, created_at
    FROM users
    WHERE created_at > NOW() - INTERVAL '7 days'
    ORDER BY created_at DESC
    LIMIT 10
    ```

    **Example response:**
    ```
    [
      {"id": 1234, "email": "user@example.com", "created_at": "2025-01-03 14:23:45"},
      {"id": 1235, "email": "another@example.com", "created_at": "2025-01-03 10:15:22"}
    ]
    ```

    <Warning>
    **Production Safety**: Always use `--access-mode restricted` for production databases. This prevents accidental data modifications and enforces query timeouts.
    </Warning>

    <Tip>
    You rarely need to use this tool directly. Instead, ask questions in natural language and let AI generate optimized SQL for you.
    </Tip>
  </Accordion>

  <Accordion title="explain_query - Analyze Query Performance" icon="chart-line">
    Explains how PostgreSQL will execute a query, showing the execution plan, cost estimates, and performance characteristics. Supports hypothetical index simulation.

    **What it does:**
    - Generates execution plans with EXPLAIN
    - Shows estimated vs actual costs with ANALYZE
    - Simulates hypothetical indexes using HypoPG extension
    - Reveals sequential scans, index usage, and join strategies
    - Identifies performance bottlenecks before execution

    **Parameters:**
    | Parameter | Type | Required | Default | Description |
    |-----------|------|----------|---------|-------------|
    | `sql` | string | Yes | - | SQL query to explain |
    | `analyze` | boolean | No | false | Run query for real statistics (takes longer but more accurate) |
    | `hypothetical_indexes` | array | No | [] | Simulated indexes to test performance impact |

    **Hypothetical Index Format:**
    Each index object requires:
    - `table`: Table name (e.g., "users")
    - `columns`: Array of column names (e.g., ["email"] or ["last_name", "first_name"])
    - `using`: Optional index method (default: "btree", options: "hash", "gist", "gin", etc.)

    **Natural language examples:**
    - "Explain how this query will run: SELECT * FROM users WHERE email = 'test@example.com'"
    - "Show me the execution plan with analysis for my orders query"
    - "What would happen if I added an index on users.email?"
    - "Simulate an index on (last_name, first_name) and show the performance impact"

    **Example - Basic explain:**
    ```
    QUERY PLAN:
    Seq Scan on users  (cost=0.00..458.00 rows=100 width=532)
      Filter: (email = 'test@example.com'::text)
    ```

    **Example - With hypothetical index:**
    Request:
    ```json
    {
      "sql": "SELECT * FROM users WHERE email = 'test@example.com'",
      "hypothetical_indexes": [
        {"table": "users", "columns": ["email"], "using": "btree"}
      ]
    }
    ```
    Result:
    ```
    QUERY PLAN (with hypothetical index):
    Index Scan using <hypothetical_index> on users  (cost=0.29..8.31 rows=1 width=532)
      Index Cond: (email = 'test@example.com'::text)

    Cost reduced from 458.00 to 8.31 (98% improvement)
    ```

    <Info>
    **Requires HypoPG Extension**: Hypothetical indexes need the `hypopg` extension installed. The tool will notify you if it's missing.
    </Info>

    <Tip>
    Use `analyze=false` (default) for quick estimates without query execution. Use `analyze=true` only when you need precise actual statistics.
    </Tip>
  </Accordion>

  <Accordion title="get_top_queries - Find Performance Bottlenecks" icon="ranking-star">
    Identifies the slowest and most resource-intensive queries using PostgreSQL's `pg_stat_statements` extension.

    **What it does:**
    - Analyzes query statistics from pg_stat_statements
    - Ranks queries by total time, mean time, or resource consumption
    - Shows execution counts, average time, and resource usage
    - Identifies queries that would benefit most from optimization
    - Provides normalized query text (parameters replaced)

    **Parameters:**
    | Parameter | Type | Required | Default | Description |
    |-----------|------|----------|---------|-------------|
    | `sort_by` | string | No | "resources" | Ranking criteria: "total_time", "mean_time", or "resources" |
    | `limit` | integer | No | 10 | Number of queries to return (for total_time/mean_time) |

    **Sort Options:**
    - **"resources"**: Comprehensive resource analysis (CPU, I/O, memory, calls)
    - **"total_time"**: Queries with highest cumulative execution time
    - **"mean_time"**: Queries with highest average execution time per call

    **Natural language examples:**
    - "What are my slowest queries?"
    - "Show me the most resource-intensive database operations"
    - "Which queries have the highest average execution time?"
    - "Find queries that run most frequently"

    **Example response (resources mode):**
    ```
    Top Resource-Intensive Queries:

    1. Query: SELECT * FROM orders o JOIN users u ON o.user_id = u.id WHERE o.created_at > $1
       Calls: 45,234
       Total Time: 1,234.56 ms
       Mean Time: 27.31 ms
       Rows Retrieved: 2,145,678

    2. Query: SELECT COUNT(*) FROM products WHERE category = $1 AND price > $2
       Calls: 12,456
       Total Time: 892.34 ms
       Mean Time: 71.64 ms
       Rows Retrieved: 124,560
    ```

    <Warning>
    **Requires pg_stat_statements Extension**: This tool depends on the `pg_stat_statements` extension being installed and enabled in your PostgreSQL configuration.
    </Warning>

    <Tip>
    Use "resources" mode (default) for general optimization. It balances execution time, frequency, and resource usage to find the most impactful optimization opportunities.
    </Tip>
  </Accordion>

  <Accordion title="analyze_workload_indexes - Optimize Database Performance" icon="gauge-high">
    Analyzes your database workload and recommends optimal indexes using industrial-strength algorithms from Microsoft SQL Server.

    **What it does:**
    - Examines frequently executed queries from pg_stat_statements
    - Applies Microsoft SQL Server DTA (Database Tuning Advisor) algorithm
    - Recommends indexes that provide maximum performance improvement
    - Considers index size limits and resource constraints
    - Provides CREATE INDEX statements ready to execute
    - Alternative LLM-based optimization available

    **Parameters:**
    | Parameter | Type | Required | Default | Description |
    |-----------|------|----------|---------|-------------|
    | `max_index_size_mb` | integer | No | 10000 | Maximum size for recommended indexes (in MB) |
    | `method` | string | No | "dta" | Analysis method: "dta" or "llm" |

    **Analysis Methods:**
    - **"dta"**: Microsoft SQL Server Database Tuning Advisor algorithm - proven, deterministic, production-ready
    - **"llm"**: AI-powered optimization - experimental, considers broader context

    **Natural language examples:**
    - "Analyze my database and recommend indexes"
    - "What indexes would improve my query performance?"
    - "Optimize my database for better speed"
    - "Find the best indexes for my workload"

    **Example response:**
    ```
    Workload Analysis Complete
    Analyzed: 45 unique queries from pg_stat_statements

    Recommended Indexes (5):

    1. CREATE INDEX idx_users_email ON users (email);
       Estimated Impact: 87% faster queries
       Affected Queries: 12
       Size: 45 MB

    2. CREATE INDEX idx_orders_user_created ON orders (user_id, created_at);
       Estimated Impact: 92% faster queries
       Affected Queries: 8
       Size: 128 MB

    3. CREATE INDEX idx_products_category_price ON products (category, price);
       Estimated Impact: 76% faster queries
       Affected Queries: 6
       Size: 67 MB

    Total Storage Required: 240 MB
    Estimated Overall Performance Improvement: 73%
    ```

    <Info>
    The DTA algorithm is the same technology used in Microsoft SQL Server's query optimization. It has been proven in enterprise production environments.
    </Info>

    <Tip>
    Run this analysis during low-traffic periods. It examines historical query patterns, so more execution history provides better recommendations.
    </Tip>
  </Accordion>

  <Accordion title="analyze_query_indexes - Optimize Specific Queries" icon="bullseye">
    Analyzes specific SQL queries (up to 10) and recommends optimal indexes tailored to those queries.

    **What it does:**
    - Analyzes provided queries instead of workload history
    - Applies DTA algorithm or LLM optimization
    - Recommends indexes optimized for specific query patterns
    - Perfect for optimizing new features before deployment
    - Simulates index impact on query execution plans

    **Parameters:**
    | Parameter | Type | Required | Default | Description |
    |-----------|------|----------|---------|-------------|
    | `queries` | array | Yes | - | List of SQL queries to analyze (1-10 queries) |
    | `max_index_size_mb` | integer | No | 10000 | Maximum size for recommended indexes (in MB) |
    | `method` | string | No | "dta" | Analysis method: "dta" or "llm" |

    **Natural language examples:**
    - "Optimize these queries: [query list]"
    - "What indexes would help this specific query?"
    - "I have 3 slow queries, recommend indexes for them"
    - "Analyze these queries and suggest performance improvements"

    **Example request:**
    ```json
    {
      "queries": [
        "SELECT * FROM users WHERE email = $1",
        "SELECT * FROM orders WHERE user_id = $1 AND created_at > $2",
        "SELECT COUNT(*) FROM products WHERE category = $1 AND price > $2"
      ],
      "max_index_size_mb": 5000,
      "method": "dta"
    }
    ```

    **Example response:**
    ```
    Query-Specific Index Recommendations

    For Query 1: SELECT * FROM users WHERE email = $1
    Recommendation: CREATE INDEX idx_users_email ON users (email);
    Impact: Sequential scan → Index scan (98.2% faster)

    For Query 2: SELECT * FROM orders WHERE user_id = $1 AND created_at > $2
    Recommendation: CREATE INDEX idx_orders_user_created ON orders (user_id, created_at);
    Impact: 345ms → 12ms (96.5% faster)

    For Query 3: SELECT COUNT(*) FROM products WHERE category = $1 AND price > $2
    Recommendation: CREATE INDEX idx_products_cat_price ON products (category, price);
    Impact: Full table scan → Index-only scan (94.1% faster)

    Total Storage: 156 MB
    Overall Performance Gain: 96.3%
    ```

    <Warning>
    Limited to 10 queries per analysis. For larger workloads, use `analyze_workload_indexes` instead.
    </Warning>

    <Tip>
    Use this before deploying new features. Analyze your new queries to proactively add optimal indexes.
    </Tip>
  </Accordion>

  <Accordion title="analyze_db_health - Monitor Database Health" icon="heart-pulse">
    Performs comprehensive health checks on your PostgreSQL database, identifying potential issues before they become problems.

    **What it does:**
    - Checks index health (invalid, duplicate, bloated indexes)
    - Monitors connection utilization and connection pool status
    - Analyzes vacuum health and transaction ID wraparound risk
    - Validates sequences approaching maximum values
    - Checks replication lag and replication slot health
    - Measures buffer cache hit rates for tables and indexes
    - Identifies invalid constraints

    **Parameters:**
    | Parameter | Type | Required | Default | Description |
    |-----------|------|----------|---------|-------------|
    | `health_type` | string | No | "all" | Health check types (comma-separated or "all") |

    **Health Check Types:**
    - **"index"**: Invalid, duplicate, and bloated indexes
    - **"connection"**: Connection count and utilization
    - **"vacuum"**: Vacuum health and transaction wraparound risk
    - **"sequence"**: Sequences near maximum values
    - **"replication"**: Replication lag and slot health
    - **"buffer"**: Buffer cache hit rates
    - **"constraint"**: Invalid constraints
    - **"all"**: Runs all checks (default)

    **Natural language examples:**
    - "Check my database health"
    - "Are there any problems with my database?"
    - "Check index health"
    - "Monitor connection utilization"
    - "Is my database at risk of transaction ID wraparound?"

    **Example response:**
    ```
    Database Health Report

    ✅ INDEX HEALTH
    - Valid indexes: 47
    - Invalid indexes: 1 (users_email_idx - needs rebuild)
    - Duplicate indexes: 2 (consider removing)
    - Bloated indexes: 3 (total waste: 456 MB)

    ✅ CONNECTION HEALTH
    - Active connections: 45 / 100 (45%)
    - Idle connections: 12
    - Status: Healthy

    ⚠️ VACUUM HEALTH
    - Last autovacuum: 3 days ago
    - Transaction ID age: 1.2B (58% to wraparound)
    - Recommendation: Schedule manual VACUUM soon

    ✅ BUFFER CACHE
    - Table hit rate: 99.2% (Excellent)
    - Index hit rate: 98.7% (Excellent)

    ⚠️ REPLICATION
    - Replication lag: 234ms (Moderate)
    - Replication slots: 2 active
    - Status: Monitor closely

    Overall Status: Healthy with 2 warnings
    ```

    <Info>
    Run health checks regularly (daily or weekly) to catch issues early. Set up automated alerts for critical problems.
    </Info>

    <Tip>
    Use specific health types during incident response. For example, `health_type="connection"` when investigating connection pool issues.
    </Tip>
  </Accordion>
</AccordionGroup>

## Authentication & Configuration

### Connection Configuration

PostgreSQL requires database credentials to establish connections. You can provide connection information in two ways:

**Option 1: Connection URI (Recommended)**
```
postgresql://username:password@hostname:port/database
```

**Option 2: Individual Parameters**
Configure in Studio environment variables:
- `PGHOST`: Database hostname
- `PGPORT`: Port number (default: 5432)
- `PGDATABASE`: Database name
- `PGUSER`: Username
- `PGPASSWORD`: Password

**Access Modes**

The server supports two operational modes:

<Tabs>
  <Tab title="Restricted Mode (Production)">
    **Recommended for production environments**

    Enable with: `--access-mode restricted`

    **Features:**
    - ✅ Read-only SQL execution (SELECT only)
    - ✅ 30-second query timeout
    - ✅ Blocks destructive operations (DROP, DELETE, UPDATE)
    - ✅ Safe for production data
    - ✅ All analysis tools available

    **Perfect for:**
    - Production database analysis
    - Data exploration by non-DBAs
    - Reporting and analytics
    - AI-powered data queries
  </Tab>

  <Tab title="Unrestricted Mode (Development)">
    **Use only in development environments**

    Enable with: `--access-mode unrestricted` (default)

    **Features:**
    - ✅ Full SQL execution (INSERT, UPDATE, DELETE, DROP)
    - ✅ No query timeouts
    - ✅ Complete database access
    - ⚠️ Can modify or delete data

    **Perfect for:**
    - Local development
    - Testing environments
    - Database setup and migrations
    - Non-production databases only
  </Tab>
</Tabs>

### Managed PostgreSQL Services

The server works seamlessly with popular managed PostgreSQL providers:

<AccordionGroup>
  <Accordion title="Supabase" icon="database">
    **Connection Setup:**
    1. Log in to your Supabase dashboard
    2. Navigate to Project Settings → Database
    3. Find "Connection String" section
    4. Copy the URI (Direct connection recommended for MCP)
    5. Replace `[YOUR-PASSWORD]` with your database password

    **Example URI:**
    ```
    postgresql://postgres:[YOUR-PASSWORD]@db.xxxxxxxxxxxx.supabase.co:5432/postgres
    ```

    **Required Extensions:**
    Supabase includes `pg_stat_statements` by default. For index optimization:
    ```sql
    CREATE EXTENSION IF NOT EXISTS hypopg;
    ```

    <Tip>
    Use Supabase's connection pooler for better performance with multiple Studio users.
    </Tip>
  </Accordion>

  <Accordion title="Neon" icon="leaf">
    **Connection Setup:**
    1. Open your Neon console
    2. Select your project and branch
    3. Go to Dashboard → Connection Details
    4. Copy the connection string

    **Example URI:**
    ```
    postgresql://username:password@ep-xxxx-xxxx.us-east-2.aws.neon.tech/neondb
    ```

    **Features:**
    - ✅ Instant branching for safe testing
    - ✅ Auto-scaling based on usage
    - ✅ Built-in connection pooling

    <Info>
    Neon's branching feature is perfect for testing index recommendations before applying to production.
    </Info>
  </Accordion>

  <Accordion title="Amazon RDS PostgreSQL" icon="aws">
    **Connection Setup:**
    1. Open AWS RDS Console
    2. Select your PostgreSQL instance
    3. Find "Connectivity & security" section
    4. Copy the endpoint URL
    5. Construct connection string

    **Example URI:**
    ```
    postgresql://username:password@database-1.xxxx.us-east-1.rds.amazonaws.com:5432/postgres
    ```

    **Enable Extensions:**
    ```sql
    CREATE EXTENSION IF NOT EXISTS pg_stat_statements;
    CREATE EXTENSION IF NOT EXISTS hypopg;
    ```

    **Security:**
    - Configure security groups to allow Studio IP addresses
    - Use IAM database authentication for enhanced security
    - Enable SSL/TLS connections
  </Accordion>

  <Accordion title="Google Cloud SQL" icon="google">
    **Connection Setup:**
    1. Open Cloud SQL Console
    2. Select your PostgreSQL instance
    3. Navigate to Connections
    4. Get public IP address or use Cloud SQL Proxy
    5. Construct connection string

    **Example URI:**
    ```
    postgresql://username:password@34.xxx.xxx.xxx:5432/postgres
    ```

    **Recommended:**
    Use Cloud SQL Proxy for secure connections without exposing public IPs.
  </Accordion>

  <Accordion title="DigitalOcean Managed Databases" icon="droplet">
    **Connection Setup:**
    1. Open DigitalOcean Control Panel
    2. Navigate to Databases
    3. Select your PostgreSQL cluster
    4. Go to Connection Details
    5. Copy the connection string (sslmode=require included)

    **Example URI:**
    ```
    postgresql://username:password@db-postgresql-nyc1-xxxxx.ondigitalocean.com:25060/defaultdb?sslmode=require
    ```
  </Accordion>

  <Accordion title="Azure Database for PostgreSQL" icon="microsoft">
    **Connection Setup:**
    1. Open Azure Portal
    2. Navigate to your PostgreSQL server
    3. Select Connection strings under Settings
    4. Copy the connection string

    **Example URI:**
    ```
    postgresql://username@servername:password@servername.postgres.database.azure.com:5432/postgres?sslmode=require
    ```

    **Note:** Azure requires `@servername` suffix in username.
  </Accordion>
</AccordionGroup>

### Self-Hosted PostgreSQL

<AccordionGroup>
  <Accordion title="Local PostgreSQL Installation" icon="laptop">
    **Default Connection:**
    ```
    postgresql://postgres:password@localhost:5432/postgres
    ```

    **Install Required Extensions:**
    ```sql
    -- Enable query statistics
    CREATE EXTENSION IF NOT EXISTS pg_stat_statements;

    -- Enable hypothetical index simulation
    CREATE EXTENSION IF NOT EXISTS hypopg;
    ```

    **Configure pg_stat_statements:**
    Edit `postgresql.conf`:
    ```
    shared_preload_libraries = 'pg_stat_statements'
    pg_stat_statements.track = all
    ```

    Restart PostgreSQL after configuration changes.
  </Accordion>

  <Accordion title="Docker PostgreSQL" icon="docker">
    **Quick Start:**
    ```bash
    docker run -d \
      --name postgres-mcp \
      -e POSTGRES_PASSWORD=yourpassword \
      -e POSTGRES_DB=mydb \
      -p 5432:5432 \
      postgres:16
    ```

    **Connection URI:**
    ```
    postgresql://postgres:yourpassword@localhost:5432/mydb
    ```

    **Enable Extensions:**
    ```bash
    docker exec -it postgres-mcp psql -U postgres -d mydb
    ```

    Then run:
    ```sql
    CREATE EXTENSION pg_stat_statements;
    CREATE EXTENSION hypopg;
    ```
  </Accordion>

  <Accordion title="Network Security" icon="shield">
    **Firewall Configuration:**

    Allow PostgreSQL port (default 5432):
    ```bash
    # Ubuntu/Debian
    sudo ufw allow 5432/tcp

    # CentOS/RHEL
    sudo firewall-cmd --permanent --add-port=5432/tcp
    sudo firewall-cmd --reload
    ```

    **PostgreSQL Authentication:**

    Edit `pg_hba.conf` to control access:
    ```
    # Allow Studio connections with password
    host    all    all    0.0.0.0/0    md5
    ```

    <Warning>
    Never expose PostgreSQL directly to the internet without proper security measures. Use VPNs, SSH tunnels, or private networks.
    </Warning>
  </Accordion>
</AccordionGroup>

### Security Best Practices

<CardGroup cols={2}>
  <Card title="Create Read-Only Users" icon="user-lock">
    For production Studio access, create dedicated read-only users:

    ```sql
    -- Create read-only user
    CREATE USER studio_reader WITH PASSWORD 'secure_password';

    -- Grant read access to all tables
    GRANT CONNECT ON DATABASE mydb TO studio_reader;
    GRANT USAGE ON SCHEMA public TO studio_reader;
    GRANT SELECT ON ALL TABLES IN SCHEMA public TO studio_reader;

    -- Ensure future tables are readable
    ALTER DEFAULT PRIVILEGES IN SCHEMA public
    GRANT SELECT ON TABLES TO studio_reader;
    ```

    Use this user with `--access-mode restricted` for maximum safety.
  </Card>

  <Card title="Enable SSL/TLS Encryption" icon="lock">
    Always use encrypted connections for production:

    **Connection URI with SSL:**
    ```
    postgresql://user:pass@host:5432/db?sslmode=require
    ```

    **SSL Modes:**
    - `require`: Encrypt connection (recommended minimum)
    - `verify-ca`: Encrypt and verify server certificate
    - `verify-full`: Encrypt, verify certificate, and hostname

    Most managed services enable SSL by default.
  </Card>

  <Card title="Use Connection Pooling" icon="layer-group">
    For multiple users or high query volume, use connection pooling:

    **PgBouncer Example:**
    ```
    postgresql://user:pass@pgbouncer-host:6432/db
    ```

    Benefits:
    - Reduces connection overhead
    - Better resource utilization
    - Protects against connection exhaustion
  </Card>

  <Card title="Audit and Monitoring" icon="eye">
    Track database access with logging:

    **Enable Query Logging:**
    ```sql
    ALTER SYSTEM SET log_statement = 'all';
    ALTER SYSTEM SET log_duration = on;
    SELECT pg_reload_conf();
    ```

    **Monitor with pg_stat_statements:**
    Queries executed through Studio are tracked, allowing you to audit all AI-generated SQL.
  </Card>
</CardGroup>

### Required PostgreSQL Extensions

For full functionality, install these extensions:

| Extension | Required For | Installation |
|-----------|-------------|--------------|
| `pg_stat_statements` | Query analysis, workload optimization, top queries | `CREATE EXTENSION pg_stat_statements;` |
| `hypopg` | Hypothetical index simulation in explain plans | `CREATE EXTENSION hypopg;` |

<Info>
The server will notify you if required extensions are missing when you attempt to use advanced features.
</Info>

## Example Workflows

<Tabs>
  <Tab title="Explore Unknown Database">
    **Scenario:** You've been given access to a new database and need to understand its structure.

    **Natural language workflow:**

    1. **"What schemas exist in this database?"**
       - Server uses `list_schemas` to show all database schemas
       - Identifies user schemas vs system schemas

    2. **"What tables are in the public schema?"**
       - Server uses `list_objects` with schema_name="public"
       - Lists all tables with their types

    3. **"Show me the structure of the users table"**
       - Server uses `get_object_details` for the users table
       - Displays columns, data types, constraints, and indexes

    4. **"What are the first 10 users?"**
       - Server generates: `SELECT * FROM public.users LIMIT 10`
       - Executes query and returns results

    **Result:** Complete understanding of database structure without writing a single SQL query.
  </Tab>

  <Tab title="Find Slow Queries">
    **Scenario:** Your application is slow and you need to identify performance bottlenecks.

    **Natural language workflow:**

    1. **"What are my slowest database queries?"**
       - Server uses `get_top_queries` with sort_by="resources"
       - Shows queries ranked by resource consumption

    2. **"Explain how this query runs: SELECT * FROM orders WHERE user_id = 123"**
       - Server uses `explain_query` to show execution plan
       - Reveals sequential scan on orders table (expensive!)

    3. **"What if I added an index on orders.user_id?"**
       - Server uses `explain_query` with hypothetical_indexes
       - Simulates index and shows 95% cost reduction

    4. **"Analyze my database and recommend indexes"**
       - Server uses `analyze_workload_indexes`
       - Provides CREATE INDEX statements for optimal performance

    **Result:** Identified bottlenecks and received concrete optimization recommendations - all through conversation.
  </Tab>

  <Tab title="Optimize New Feature">
    **Scenario:** You're deploying a new feature with specific queries and want to ensure good performance.

    **Natural language workflow:**

    1. **"I have these 3 queries for my new feature: [query list]. What indexes should I create?"**
       - Provide your queries:
         ```sql
         SELECT * FROM products WHERE category = 'electronics' AND price > 100
         SELECT COUNT(*) FROM orders WHERE created_at > NOW() - INTERVAL '7 days'
         SELECT u.*, COUNT(o.id) FROM users u LEFT JOIN orders o ON u.id = o.user_id GROUP BY u.id
         ```

    2. Server uses `analyze_query_indexes` with your queries
       - Applies DTA algorithm to each query
       - Returns optimized index recommendations:
         ```sql
         CREATE INDEX idx_products_cat_price ON products (category, price);
         CREATE INDEX idx_orders_created ON orders (created_at);
         CREATE INDEX idx_orders_user ON orders (user_id);
         ```

    3. **"Explain my first query with the recommended index"**
       - Server uses `explain_query` with hypothetical index
       - Shows performance improvement: 340ms → 8ms (97.6% faster)

    4. **"Create those indexes"** (in unrestricted mode)
       - Server executes CREATE INDEX statements
       - Indexes are created and queries are optimized

    **Result:** New feature deployed with optimal performance from day one.
  </Tab>

  <Tab title="Data Analysis">
    **Scenario:** Marketing team needs sales insights but doesn't know SQL.

    **Natural language workflow:**

    1. **"How many orders were placed in the last 30 days?"**
       - Server generates: `SELECT COUNT(*) FROM orders WHERE created_at > NOW() - INTERVAL '30 days'`
       - Returns: 4,523 orders

    2. **"What's the average order value by week?"**
       - Server generates complex query with date grouping
       - Returns weekly averages with dates

    3. **"Which products had more than 100 sales last month?"**
       - Server generates:
         ```sql
         SELECT p.name, COUNT(oi.id) as sales_count, SUM(oi.quantity * oi.price) as revenue
         FROM products p
         JOIN order_items oi ON p.id = oi.product_id
         JOIN orders o ON oi.order_id = o.id
         WHERE o.created_at > NOW() - INTERVAL '1 month'
         GROUP BY p.id, p.name
         HAVING COUNT(oi.id) > 100
         ORDER BY sales_count DESC
         ```

    4. **"Show me customers who haven't ordered in 6 months"**
       - Server generates query with LEFT JOIN to find inactive customers
       - Returns list with customer details and last order date

    **Result:** Marketing team gets instant insights without bothering engineering or learning SQL.
  </Tab>

  <Tab title="Database Health Monitoring">
    **Scenario:** Weekly database health check to prevent issues.

    **Natural language workflow:**

    1. **"Check my database health"**
       - Server uses `analyze_db_health` with health_type="all"
       - Runs comprehensive health analysis

    2. Server reports:
       - ✅ **Indexes**: 2 bloated indexes found (300 MB waste)
       - ⚠️ **Connections**: 78/100 connections used (78% - approaching limit)
       - ✅ **Vacuum**: Last autovacuum 6 hours ago (healthy)
       - ⚠️ **Buffer Cache**: 85% hit rate (could be improved)
       - ✅ **Replication**: 45ms lag (healthy)

    3. **"How can I improve the buffer cache hit rate?"**
       - Server suggests increasing `shared_buffers` parameter
       - Recommends analyzing working set size

    4. **"Which indexes are bloated?"**
       - Server provides detailed list with REINDEX recommendations

    5. **"What's using all my connections?"**
       - Server queries pg_stat_activity
       - Shows connection breakdown by application and state

    **Result:** Proactive identification of issues before they impact users.
  </Tab>

  <Tab title="Query Troubleshooting">
    **Scenario:** A specific query is timing out in production.

    **Natural language workflow:**

    1. **"This query is timing out: SELECT * FROM orders o JOIN users u ON o.user_id = u.id WHERE o.status = 'pending' AND u.country = 'US' ORDER BY o.created_at DESC"**

    2. **"Explain why this query is slow"**
       - Server uses `explain_query` with analyze=true
       - Execution plan shows:
         - Sequential scan on orders (48,000 rows)
         - Sequential scan on users (125,000 rows)
         - Hash join taking 2,300ms

    3. **"What indexes would make this faster?"**
       - Server uses `analyze_query_indexes`
       - Recommends:
         ```sql
         CREATE INDEX idx_orders_status_created ON orders (status, created_at DESC);
         CREATE INDEX idx_users_country_id ON users (country, id);
         ```

    4. **"Simulate those indexes and show me the improvement"**
       - Server uses `explain_query` with hypothetical_indexes
       - New plan shows:
         - Index scan on orders (234 rows)
         - Index scan on users (45,000 rows)
         - Nested loop join taking 45ms
         - **Improvement: 98.1% faster (2,300ms → 45ms)**

    5. **"Create those indexes"** (after approval)
       - Indexes created in production
       - Query performance problem solved

    **Result:** Query timeout resolved with data-driven index recommendations.
  </Tab>

  <Tab title="Schema Investigation">
    **Scenario:** Planning a migration and need to understand table relationships.

    **Natural language workflow:**

    1. **"What tables are related to the users table?"**
       - Server analyzes foreign key constraints
       - Finds: orders, user_preferences, user_sessions, reviews

    2. **"Show me all foreign keys pointing to users"**
       - Server queries information_schema
       - Lists all referencing tables and columns

    3. **"What's the structure of the orders table with all its constraints?"**
       - Server uses `get_object_details` for orders
       - Shows columns, foreign keys, indexes, check constraints

    4. **"How many orders does each user have?"**
       - Server generates:
         ```sql
         SELECT u.id, u.email, COUNT(o.id) as order_count
         FROM users u
         LEFT JOIN orders o ON u.id = o.user_id
         GROUP BY u.id, u.email
         ORDER BY order_count DESC
         ```

    5. **"Are there any users without orders?"**
       - Server generates query with LEFT JOIN and NULL check
       - Returns orphaned users

    **Result:** Complete understanding of schema relationships for safe migration planning.
  </Tab>
</Tabs>

## Troubleshooting

<AccordionGroup>
  <Accordion title="Error: No database URL provided" icon="circle-exclamation">
    **Problem:** Server cannot connect because DATABASE_URI is not configured.

    **Solution:**
    1. Open NimbleBrain Studio
    2. Navigate to Connections → PostgreSQL connection
    3. Click "Configure" or "Edit"
    4. Add environment variable:
       - Key: `DATABASE_URI`
       - Value: Your full connection string
    5. Save and restart the server

    **Example connection strings:**
    - Local: `postgresql://postgres:password@localhost:5432/mydb`
    - Supabase: `postgresql://postgres:password@db.xxx.supabase.co:5432/postgres`
    - Neon: `postgresql://user:password@ep-xxx.neon.tech/neondb`
  </Accordion>

  <Accordion title="Error: Could not connect to database" icon="plug">
    **Problem:** Connection string is provided but connection fails.

    **Common causes:**

    1. **Incorrect credentials**
       - Verify username and password
       - Check for special characters that need URL encoding

    2. **Network/Firewall blocking connection**
       - Verify database server is reachable
       - Check firewall rules allow port 5432
       - For cloud databases, verify IP allowlist

    3. **Wrong host or port**
       - Confirm hostname is correct
       - Verify port (default: 5432)
       - Check if using connection pooler (different port)

    4. **Database doesn't exist**
       - Verify database name in connection string
       - Create database if needed

    5. **SSL/TLS required but not enabled**
       - Add `?sslmode=require` to connection string
       - Most managed services require SSL

    **Debug steps:**
    ```bash
    # Test connection from command line
    psql "postgresql://user:pass@host:5432/db"

    # Check if server is reachable
    telnet hostname 5432
    ```
  </Accordion>

  <Accordion title="Error: pg_stat_statements extension not found" icon="puzzle-piece">
    **Problem:** Advanced query analysis features require `pg_stat_statements` extension.

    **Solution:**

    **For managed services:**
    - Most providers include this extension by default
    - Check provider documentation for enabling extensions

    **For self-hosted PostgreSQL:**

    1. Enable in postgresql.conf:
       ```
       shared_preload_libraries = 'pg_stat_statements'
       pg_stat_statements.track = all
       ```

    2. Restart PostgreSQL:
       ```bash
       sudo systemctl restart postgresql
       ```

    3. Create extension:
       ```sql
       CREATE EXTENSION pg_stat_statements;
       ```

    4. Verify installation:
       ```sql
       SELECT * FROM pg_stat_statements LIMIT 1;
       ```

    **Affected features without this extension:**
    - `get_top_queries` - Cannot identify slow queries
    - `analyze_workload_indexes` - Cannot analyze workload patterns
  </Accordion>

  <Accordion title="Error: hypopg extension not found" icon="flask">
    **Problem:** Hypothetical index simulation requires `hypopg` extension.

    **Solution:**

    **For managed services:**
    Check if provider supports hypopg:
    - Supabase: ✅ Available
    - Neon: ⚠️ Check current support
    - RDS: ✅ Available (enable in parameter group)
    - Cloud SQL: ⚠️ May need to request

    **For self-hosted PostgreSQL:**

    1. Install hypopg:
       ```bash
       # Ubuntu/Debian
       sudo apt-get install postgresql-16-hypopg

       # From source
       git clone https://github.com/HypoPG/hypopg
       cd hypopg
       make
       sudo make install
       ```

    2. Create extension:
       ```sql
       CREATE EXTENSION hypopg;
       ```

    3. Verify:
       ```sql
       SELECT * FROM hypopg_list_indexes();
       ```

    **Affected features without this extension:**
    - `explain_query` with hypothetical_indexes parameter
    - Cannot simulate index impact before creation

    <Info>
    You can still use all other features. Hypothetical indexes are optional for query explanation.
    </Info>
  </Accordion>

  <Accordion title="Query timeout in restricted mode" icon="clock">
    **Problem:** Query times out after 30 seconds in restricted mode.

    **Cause:** Restricted mode enforces 30-second timeout to protect production databases.

    **Solutions:**

    1. **Optimize the query:**
       - Use `explain_query` to identify bottlenecks
       - Use `analyze_query_indexes` to get index recommendations
       - Add recommended indexes

    2. **Add appropriate indexes:**
       - Long queries usually indicate missing indexes
       - Use index optimization tools

    3. **Use unrestricted mode (dev only):**
       - Only for non-production databases
       - Change `--access-mode` to `unrestricted`
       - No timeout enforcement

    4. **Limit result set:**
       - Add LIMIT clause to reduce rows returned
       - Use pagination for large datasets
       - Filter data before aggregation

    **Example optimization workflow:**
    ```
    "This query is timing out: SELECT * FROM large_table WHERE status = 'active'"
    → Server explains query, shows sequential scan
    → "What index would help?"
    → Server recommends: CREATE INDEX idx_large_table_status ON large_table(status)
    → Create index
    → Query now completes in <1 second
    ```
  </Accordion>

  <Accordion title="Access denied errors" icon="ban">
    **Problem:** Permission denied when executing queries or accessing tables.

    **Common scenarios:**

    1. **User lacks SELECT permissions:**
       ```sql
       -- Grant read access to schema
       GRANT USAGE ON SCHEMA public TO your_user;

       -- Grant read access to all tables
       GRANT SELECT ON ALL TABLES IN SCHEMA public TO your_user;

       -- Grant access to future tables
       ALTER DEFAULT PRIVILEGES IN SCHEMA public
       GRANT SELECT ON TABLES TO your_user;
       ```

    2. **Trying to modify data in restricted mode:**
       - Restricted mode blocks INSERT, UPDATE, DELETE, DROP
       - This is intentional for production safety
       - Use unrestricted mode only in development

    3. **Row-level security (RLS) policies:**
       - Some managed services (Supabase) use RLS
       - Create policies or use service role key
       - Or disable RLS for MCP user:
         ```sql
         ALTER TABLE your_table DISABLE ROW LEVEL SECURITY;
         ```

    4. **Schema-level permissions:**
       - Verify user has access to target schema
       - Check `pg_namespace` privileges
  </Accordion>

  <Accordion title="Connection pool exhausted" icon="circle-xmark">
    **Problem:** "Sorry, too many clients already" or connection pool errors.

    **Cause:** PostgreSQL has a limited number of connections (default: 100).

    **Solutions:**

    1. **Use connection pooling:**
       - Deploy PgBouncer or similar pooler
       - Point Studio to pooler instead of direct database
       - Pooler reuses connections efficiently

    2. **Increase max_connections:**
       ```sql
       -- Check current setting
       SHOW max_connections;

       -- Increase (requires restart)
       ALTER SYSTEM SET max_connections = 200;
       -- Then restart PostgreSQL
       ```

       **Warning:** More connections = more memory usage

    3. **Close idle connections:**
       ```sql
       -- Find idle connections
       SELECT pid, usename, state, state_change
       FROM pg_stat_activity
       WHERE state = 'idle'
       AND state_change < NOW() - INTERVAL '1 hour';

       -- Terminate specific connection
       SELECT pg_terminate_backend(pid);
       ```

    4. **For managed services:**
       - Use built-in connection pooling
       - Upgrade to plan with more connections
       - Enable connection pooler mode
  </Accordion>

  <Accordion title="Slow index recommendations" icon="hourglass">
    **Problem:** `analyze_workload_indexes` takes a very long time or times out.

    **Cause:** DTA algorithm analyzes large query workloads and tests multiple index combinations.

    **Solutions:**

    1. **Use during off-peak hours:**
       - Run analysis when database is less busy
       - Schedule as weekly or monthly task

    2. **Reduce max_index_size_mb:**
       - Lower value = faster analysis
       - Try `max_index_size_mb=1000` (1GB) instead of default 10GB

    3. **Use analyze_query_indexes for specific queries:**
       - Instead of analyzing entire workload
       - Target problematic queries directly
       - Faster and more focused

    4. **Ensure pg_stat_statements has data:**
       ```sql
       -- Check number of queries tracked
       SELECT COUNT(*) FROM pg_stat_statements;
       ```

       - If count is low, wait for more query history
       - Reset stats if they're stale: `SELECT pg_stat_statements_reset();`

    5. **Try LLM method for faster results:**
       - Use `method="llm"` instead of `method="dta"`
       - Faster but less deterministic
       - Good for initial exploration
  </Accordion>

  <Accordion title="SSL/TLS certificate errors" icon="certificate">
    **Problem:** SSL verification fails or certificate errors.

    **Error messages:**
    - "SSL connection could not be established"
    - "certificate verify failed"
    - "self-signed certificate"

    **Solutions:**

    1. **Adjust SSL mode:**
       - `sslmode=require` - Encrypt but don't verify certificate
       - `sslmode=verify-ca` - Verify certificate authority
       - `sslmode=verify-full` - Full verification (most secure)

       Example:
       ```
       postgresql://user:pass@host:5432/db?sslmode=require
       ```

    2. **For self-signed certificates:**
       ```
       postgresql://user:pass@host:5432/db?sslmode=require&sslrootcert=/path/to/ca.crt
       ```

    3. **For development (not recommended for production):**
       ```
       postgresql://user:pass@host:5432/db?sslmode=disable
       ```

       <Warning>
       Never disable SSL for production databases. This exposes your data to interception.
       </Warning>

    4. **For managed services:**
       - Download CA certificate from provider
       - Add certificate to connection string
       - Supabase, Neon, RDS all provide CA certificates
  </Accordion>

  <Accordion title="Unexpected query results" icon="triangle-exclamation">
    **Problem:** Query returns unexpected data or wrong number of rows.

    **Debug steps:**

    1. **Check the generated SQL:**
       - Ask: "Show me the SQL you generated for that query"
       - Verify it matches your intent
       - Look for missing WHERE clauses or incorrect JOINs

    2. **Examine table structure:**
       - Use `get_object_details` to see column names
       - Verify data types match expectations
       - Check for unexpected NULLs

    3. **Test with simple queries:**
       - Start with: `SELECT COUNT(*) FROM table`
       - Add filters incrementally
       - Isolate the issue

    4. **Check for case sensitivity:**
       - PostgreSQL is case-sensitive for quoted identifiers
       - Column "Name" ≠ column "name"
       - Use lowercase for consistency

    5. **Verify schema context:**
       - Ensure you're querying the right schema
       - Specify schema explicitly: `schema.table`
       - Use `list_schemas` and `list_objects` to confirm

    <Tip>
    When in doubt, request the exact SQL generated and review it. Natural language interpretation can sometimes miss nuances.
    </Tip>
  </Accordion>
</AccordionGroup>

## Links & Resources

<CardGroup cols={2}>
  <Card title="GitHub Repository" icon="github" href="https://github.com/crystaldba/postgres-mcp">
    View source code, report issues, and contribute to development
  </Card>

  <Card title="PostgreSQL Documentation" icon="book" href="https://www.postgresql.org/docs/current/">
    Official PostgreSQL documentation and SQL reference
  </Card>

  <Card title="pg_stat_statements" icon="chart-bar" href="https://www.postgresql.org/docs/current/pgstatstatements.html">
    Documentation for query statistics extension
  </Card>

  <Card title="HypoPG Extension" icon="flask" href="https://github.com/HypoPG/hypopg">
    Hypothetical index simulation extension
  </Card>

  <Card title="MCP Protocol" icon="plug" href="https://modelcontextprotocol.io">
    Learn about the Model Context Protocol standard
  </Card>

  <Card title="NimbleBrain Studio" icon="plug" href="https://www.nimblebrain.ai">
    Explore more integrations and integrations
  </Card>
</CardGroup>

## Learning Resources

<AccordionGroup>
  <Accordion title="Understanding Database Indexes" icon="book-open">
    **What are indexes?**
    Indexes are data structures that speed up data retrieval by creating quick lookup paths. Like a book's index, they help find information without scanning every page.

    **When to use indexes:**
    - ✅ Columns in WHERE clauses
    - ✅ Columns in JOIN conditions
    - ✅ Columns in ORDER BY
    - ✅ Columns frequently searched

    **When NOT to use indexes:**
    - ❌ Small tables (faster to scan)
    - ❌ Columns rarely queried
    - ❌ Columns with low cardinality (few unique values)
    - ❌ Tables with heavy INSERT/UPDATE (indexes slow writes)

    **Resources:**
    - [PostgreSQL Index Types](https://www.postgresql.org/docs/current/indexes-types.html)
    - [Use The Index, Luke!](https://use-the-index-luke.com/)
  </Accordion>

  <Accordion title="Reading Execution Plans" icon="chart-line">
    **Understanding EXPLAIN output:**

    **Sequential Scan:**
    ```
    Seq Scan on users  (cost=0.00..458.00 rows=100 width=532)
    ```
    - Reads entire table
    - Slow for large tables
    - Usually means missing index

    **Index Scan:**
    ```
    Index Scan using users_email_idx on users  (cost=0.29..8.31 rows=1)
    ```
    - Uses index for fast lookup
    - Much faster than sequential scan
    - Ideal for queries with WHERE clauses

    **Key metrics:**
    - **cost**: Estimated query expense (lower = faster)
    - **rows**: Estimated rows returned
    - **width**: Average row size in bytes

    **Resources:**
    - [PostgreSQL EXPLAIN](https://www.postgresql.org/docs/current/sql-explain.html)
    - [Explain Analyzer](https://explain.dalibo.com/)
  </Accordion>

  <Accordion title="Database Health Monitoring" icon="heart-pulse">
    **Critical health indicators:**

    **1. Index Health**
    - Invalid indexes don't serve queries (need rebuild)
    - Duplicate indexes waste storage and slow writes
    - Bloated indexes need REINDEX

    **2. Connection Health**
    - Monitor connection pool utilization
    - Watch for connection leaks
    - Tune max_connections appropriately

    **3. Vacuum Health**
    - Prevents transaction ID wraparound
    - Reclaims storage from deleted rows
    - Critical for long-term stability

    **4. Buffer Cache**
    - High hit rate (>95%) = good performance
    - Low hit rate = need more memory
    - Monitor with pg_stat_database

    **5. Replication**
    - Monitor replication lag
    - Check replication slot health
    - Ensure standbys stay synchronized

    **Best practices:**
    - Run health checks weekly
    - Set up automated alerts
    - Investigate warnings promptly
    - Document baseline metrics
  </Accordion>

  <Accordion title="SQL Query Optimization" icon="gauge-high">
    **Common optimization strategies:**

    **1. Use indexes effectively**
    - Create indexes on filter columns
    - Use covering indexes for SELECT columns
    - Consider multi-column indexes for compound filters

    **2. Limit result sets**
    - Use LIMIT for pagination
    - Filter early with WHERE clauses
    - Avoid SELECT * (specify columns)

    **3. Optimize JOINs**
    - Index foreign key columns
    - Use INNER JOIN instead of subqueries when possible
    - Filter before joining when possible

    **4. Avoid query anti-patterns**
    - ❌ SELECT * FROM large_table
    - ❌ Queries without WHERE clause on large tables
    - ❌ Functions on indexed columns: WHERE LOWER(email) = 'x'
    - ❌ OR conditions spanning different columns

    **5. Use appropriate data types**
    - Smaller types = better performance
    - Use INTEGER instead of BIGINT when sufficient
    - Use VARCHAR instead of TEXT for short strings

    **Query optimization workflow:**
    1. Identify slow query with `get_top_queries`
    2. Analyze with `explain_query`
    3. Get recommendations with `analyze_query_indexes`
    4. Test with hypothetical indexes
    5. Create optimal indexes
    6. Verify improvement
  </Accordion>

  <Accordion title="PostgreSQL Access Patterns" icon="route">
    **OLTP vs OLAP workloads:**

    **OLTP (Online Transaction Processing):**
    - Characteristics: Many small, fast queries
    - Examples: User login, place order, update profile
    - Optimization: B-tree indexes, primary keys, foreign keys
    - Best for: Application databases

    **OLAP (Online Analytical Processing):**
    - Characteristics: Few large, complex queries
    - Examples: Sales reports, trend analysis, aggregations
    - Optimization: Aggregated tables, materialized views, column stores
    - Best for: Analytics, reporting

    **Mixed workloads:**
    - Use read replicas for analytics
    - Separate OLTP and OLAP databases
    - Schedule heavy queries during off-peak
    - Use connection pooling

    **Optimization for Studio:**
    - Studio queries are typically OLAP-style
    - Use restricted mode for production
    - Create read replicas for analytics
    - Add indexes for common query patterns
  </Accordion>

  <Accordion title="Advanced PostgreSQL Features" icon="star">
    **Features used by this integration:**

    **1. pg_stat_statements**
    - Tracks query execution statistics
    - Identifies slow queries automatically
    - Foundation for workload analysis
    - Enable in all databases

    **2. HypoPG**
    - Simulates hypothetical indexes
    - No storage overhead for testing
    - Critical for index recommendations
    - Safe to test in production

    **3. EXPLAIN ANALYZE**
    - Shows actual vs estimated query plans
    - Identifies planner mistakes
    - Reveals actual bottlenecks
    - Use carefully (actually runs query)

    **4. Information Schema**
    - Standard SQL metadata views
    - Database structure introspection
    - Used by all schema discovery tools
    - Always available

    **5. pg_stat views**
    - Real-time database statistics
    - Monitor connections, locks, activity
    - Foundation for health checks
    - Essential for troubleshooting

    **Other powerful features:**
    - Materialized views for pre-computed results
    - Partitioning for large tables
    - VACUUM and autovacuum for maintenance
    - Write-ahead logging for durability
    - Replication for high availability
  </Accordion>
</AccordionGroup>

---

<Note>
**Production Recommendation:** Always use `--access-mode restricted` when connecting to production databases through Studio. This prevents accidental data modifications while maintaining full access to analysis and optimization features.
</Note>
