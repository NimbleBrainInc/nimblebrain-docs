---
title: 'Tavily'
sidebarTitle: 'Tavily'
description: 'AI-powered web search and research with real-time internet access, content extraction, and intelligent web crawling'
---

## Overview

<CardGroup cols={2}>
  <Card title="What it does" icon="wand-magic-sparkles">
    Tavily provides AI-optimized web search and research capabilities specifically designed for AI agents and LLMs. Search the real-time web, extract content from specific URLs, crawl websites systematically, and map site structures - all optimized for accuracy and relevance. Results include source citations, relevance scoring, and intelligent content filtering to deliver the most useful information without noise.

    **Key Features:**
    - AI-powered web search optimized for LLMs
    - Real-time internet access for current information
    - Content extraction from specific URLs
    - Intelligent web crawling with depth control
    - Website structure mapping and analysis
    - Multiple search depths (basic/advanced)
    - Domain and time-based filtering
    - News-specific search mode
    - Image search with descriptions
    - Natural language crawler instructions
  </Card>

  <Card title="Use Cases" icon="lightbulb">
    **Research & Analysis**: Market research, competitive intelligence, academic research, trend analysis, fact-checking and verification

    **Content Discovery**: News monitoring, content research for writing, product research and comparison, technical documentation exploration

    **Data Collection**: Systematic site content extraction, structured data gathering, site architecture analysis, content auditing

    **Intelligence Gathering**: Due diligence investigations, brand monitoring, industry research, regulatory compliance research
  </Card>
</CardGroup>

<Info>
  Tavily provides AI-optimized search results specifically designed for AI agents and LLMs, filtering noise and delivering accurate, relevant information with authoritative source citations.
</Info>

## Quick Start

<Steps>
  <Step title="Get your API key">
    Sign up for a Tavily account at [app.tavily.com/sign-up](https://app.tavily.com/sign-up)

    **Free Tier Includes:**
    - 1,000 search requests/month
    - All 4 search tools available
    - Basic and advanced search depths
    - Real-time web access
    - AI-powered result optimization
    - Source citations and relevance scoring
    - No credit card required

    After signing up:
    1. Verify your email address
    2. Navigate to your API dashboard
    3. Copy your API key from the dashboard

    <Tip>
      The free tier provides 1,000 requests per month - perfect for testing and moderate research usage.
    </Tip>
  </Step>

  <Step title="Add to NimbleBrain Studio">
    In NimbleBrain Studio:

    1. Navigate to **Connections** in the sidebar
    2. Click **Add Server**
    3. Search for "Tavily" in the server registry
    4. Click **Configure**
    5. Paste your API key in the **TAVILY_API_KEY** field
    6. Click **Save & Enable**

    <Info>
      The server will automatically connect and enable real-time web search in your conversations.
    </Info>
  </Step>

  <Step title="Test your connection">
    In your Studio chat, try this prompt:

    ```text
    "Search the web for the latest news about artificial intelligence breakthroughs this week"
    ```

    You should see:
    - Real-time search results from authoritative sources
    - Content summaries and key information
    - Source URLs with publication dates
    - Relevance scores for each result
    - The üîß tool indicator confirming Tavily is working

    <Tip>
      Look for cited sources and relevance scores to confirm search quality.
    </Tip>
  </Step>
</Steps>

## Available Tools

<AccordionGroup>
  <Accordion title="tavily-search" icon="magnifying-glass">
    Powerful AI-optimized web search with comprehensive real-time results, customizable filtering, and intelligent ranking specifically designed for research and information gathering.

    **Parameters:**

    | Parameter | Type | Required | Default | Description |
    |-----------|------|----------|---------|-------------|
    | query | string | Yes | - | Search query or research question |
    | search_depth | string | No | "basic" | Search depth: "basic" (fast) or "advanced" (thorough) |
    | topic | string | No | "general" | Search category: "general" or "news" |
    | days | number | No | 3 | Days back for news search (news topic only) |
    | time_range | string | No | - | Time range: "day"/"week"/"month"/"year" or "d"/"w"/"m"/"y" |
    | start_date | string | No | "" | Start date for results (format: YYYY-MM-DD) |
    | end_date | string | No | "" | End date for results (format: YYYY-MM-DD) |
    | max_results | number | No | 10 | Maximum results (5-20) |
    | include_images | boolean | No | false | Include query-related images |
    | include_image_descriptions | boolean | No | false | Include images with descriptions |
    | include_raw_content | boolean | No | false | Include full HTML content |
    | include_domains | array | No | [] | Domains to specifically include |
    | exclude_domains | array | No | [] | Domains to exclude |
    | country | string | No | "" | Boost results from specific country (general topic only) |
    | include_favicon | boolean | No | false | Include favicon URLs |

    **Returns:**
    ```json
    {
      "query": "artificial intelligence trends 2024",
      "answer": "AI-generated summary of findings...",
      "results": [
        {
          "title": "AI Trends Report 2024",
          "url": "https://example.com/ai-trends",
          "content": "Summary of the page content...",
          "score": 0.95,
          "published_date": "2024-01-15",
          "raw_content": "Full HTML content (if requested)",
          "favicon": "https://example.com/favicon.ico"
        }
      ],
      "images": [
        {
          "url": "https://example.com/image.jpg",
          "description": "AI visualization"
        }
      ],
      "follow_up_questions": [
        "What are the key AI breakthroughs in healthcare?",
        "How is AI impacting software development?"
      ]
    }
    ```

    **Example Usage:**
    ```text
    "Search for recent developments in the electric vehicle market, focusing on Tesla's competitors"
    ```

    ```text
    "Search news from the last 7 days about climate change policy"
    ```

    ```text
    "Find technical documentation about React hooks from the official React website"
    ```

    <Tip>
      Use **basic** search depth for quick lookups (2-3 seconds), **advanced** for comprehensive research (4-6 seconds with more authoritative sources).
    </Tip>
  </Accordion>

  <Accordion title="tavily-extract" icon="file-arrow-down">
    Extract and process raw content from specific URLs with intelligent parsing. Perfect for gathering detailed information from known sources, analyzing specific articles, or collecting data from multiple pages.

    **Parameters:**

    | Parameter | Type | Required | Default | Description |
    |-----------|------|----------|---------|-------------|
    | urls | array | Yes | - | List of URLs to extract content from |
    | extract_depth | string | No | "basic" | Extraction depth: "basic" or "advanced" |
    | include_images | boolean | No | false | Include images from pages |
    | format | string | No | "markdown" | Output format: "markdown" or "text" |
    | include_favicon | boolean | No | false | Include favicon URLs |

    **Returns:**
    ```json
    {
      "results": [
        {
          "url": "https://example.com/article",
          "raw_content": "Extracted content in specified format...",
          "images": [
            "https://example.com/image1.jpg",
            "https://example.com/image2.jpg"
          ],
          "favicon": "https://example.com/favicon.ico"
        }
      ]
    }
    ```

    **Example Usage:**
    ```text
    "Extract the content from these three blog posts: [url1], [url2], [url3]"
    ```

    ```text
    "Extract all content and images from this LinkedIn profile: [linkedin-url]" (use advanced depth)
    ```

    ```text
    "Get the full text content from this research paper: [pdf-url]"
    ```

    <Tip>
      Use **advanced** extraction depth for complex pages like LinkedIn profiles, embedded content, or pages with tables and structured data. Basic depth is faster for simple articles.
    </Tip>
  </Accordion>

  <Accordion title="tavily-crawl" icon="spider">
    Systematically explore and extract content from websites starting from a base URL. The crawler intelligently follows links like a graph traversal, with configurable depth, breadth, and filtering options.

    **Parameters:**

    | Parameter | Type | Required | Default | Description |
    |-----------|------|----------|---------|-------------|
    | url | string | Yes | - | Root URL to begin the crawl |
    | max_depth | integer | No | 1 | Maximum crawl depth from base URL (minimum: 1) |
    | max_breadth | integer | No | 20 | Maximum links per page (minimum: 1) |
    | limit | integer | No | 50 | Total pages to crawl before stopping (minimum: 1) |
    | instructions | string | No | - | Natural language instructions for crawler |
    | select_paths | array | No | [] | Regex patterns for path filtering (e.g., /docs/.*) |
    | select_domains | array | No | [] | Regex patterns for domain filtering |
    | allow_external | boolean | No | true | Whether to return external links |
    | extract_depth | string | No | "basic" | Content extraction: "basic" or "advanced" |
    | format | string | No | "markdown" | Output format: "markdown" or "text" |
    | include_favicon | boolean | No | false | Include favicon URLs |

    **Returns:**
    ```json
    {
      "base_url": "https://example.com",
      "results": [
        {
          "url": "https://example.com/page1",
          "raw_content": "Extracted page content...",
          "favicon": "https://example.com/favicon.ico"
        },
        {
          "url": "https://example.com/page2",
          "raw_content": "Extracted page content..."
        }
      ],
      "response_time": 3.5
    }
    ```

    **Example Usage:**
    ```text
    "Crawl the React documentation starting from docs.react.dev and extract all content about hooks"
    ```

    ```text
    "Systematically crawl example.com/blog with max depth of 2 and extract all blog posts"
    ```

    ```text
    "Crawl the /api section of this documentation site and map all endpoints"
    ```

    <Warning>
      Be mindful of crawl limits. Large sites with high depth/breadth settings can consume many pages quickly. Start with conservative settings and adjust as needed.
    </Warning>
  </Accordion>

  <Accordion title="tavily-map" icon="sitemap">
    Create a structured map of website URLs to understand site architecture, content organization, and navigation paths. Perfect for site audits, content discovery, and analyzing website structure without extracting full content.

    **Parameters:**

    | Parameter | Type | Required | Default | Description |
    |-----------|------|----------|---------|-------------|
    | url | string | Yes | - | Root URL to begin mapping |
    | max_depth | integer | No | 1 | Maximum mapping depth (minimum: 1) |
    | max_breadth | integer | No | 20 | Maximum links per level (minimum: 1) |
    | limit | integer | No | 50 | Total links to map (minimum: 1) |
    | instructions | string | No | - | Natural language instructions for mapping |
    | select_paths | array | No | [] | Regex patterns for path selection |
    | select_domains | array | No | [] | Regex patterns for domain selection |
    | allow_external | boolean | No | true | Include external links in results |

    **Returns:**
    ```json
    {
      "base_url": "https://example.com",
      "results": [
        "https://example.com/about",
        "https://example.com/products",
        "https://example.com/products/item1",
        "https://example.com/contact",
        "https://example.com/blog"
      ],
      "response_time": 2.1
    }
    ```

    **Example Usage:**
    ```text
    "Map the structure of example.com to see all main sections and pages"
    ```

    ```text
    "Create a site map for docs.example.com focusing only on the /api paths"
    ```

    ```text
    "Map all product pages on this e-commerce site starting from /products"
    ```

    <Tip>
      Mapping is faster than crawling since it doesn't extract content. Use it first to understand site structure before deciding what to crawl.
    </Tip>
  </Accordion>
</AccordionGroup>

## Authentication

<Info>
  **API Key Required**: This server requires a Tavily API key to access web search and research capabilities.
</Info>

### Getting Your API Key

1. Create an account at [app.tavily.com/sign-up](https://app.tavily.com/sign-up)
2. Choose your plan (Free tier available with no credit card)
3. Verify your email address
4. Navigate to the API dashboard
5. Copy your API key
6. Add it to your Studio server configuration

<Tip>
  Start with the Free tier - 1,000 requests/month is sufficient for extensive testing and many research projects.
</Tip>

### Rate Limits & Pricing

| Plan | Requests/Month | Features | Price |
|------|----------------|----------|-------|
| **Free** | 1,000 | All 4 tools, basic + advanced search | $0 |
| **Basic** | 10,000 | + Priority support | $49/mo |
| **Pro** | 50,000 | + Higher rate limits, advanced features | $149/mo |
| **Enterprise** | Custom | Dedicated support, custom limits, SLA | Custom |

**Request Counting:**
- Each search query = 1 request
- Each extract call = 1 request (regardless of URL count)
- Each crawl session = 1 request (regardless of pages crawled)
- Each map operation = 1 request
- Failed requests don't count toward limit
- Limit resets on the 1st of each month

<Warning>
  Free tier is limited to 1,000 requests per month. Monitor your usage in the Tavily dashboard to avoid hitting limits.
</Warning>

### Managing Your API Key in Studio

Your API key is securely stored in NimbleBrain Studio. To update it:

1. Go to **Connections**
2. Find "Tavily" in your server list
3. Click **Edit Configuration**
4. Update your API key
5. Click **Save**

<Info>
  Studio automatically manages server connections - no manual restarts required.
</Info>

### Security Best Practices

<AccordionGroup>
  <Accordion title="Protect Your API Key" icon="lock">
    Your Tavily API key grants access to your search quota:

    - Never share your API key publicly
    - Don't commit keys to version control
    - Rotate keys periodically (every 90 days)
    - Monitor usage for unexpected activity
    - Use separate keys for different environments
    - Keep keys in secure credential managers

    <Warning>
      If your key is compromised, regenerate it immediately in the Tavily dashboard under API Settings.
    </Warning>
  </Accordion>

  <Accordion title="Monitor Usage" icon="chart-line">
    Track your API usage to avoid hitting limits:

    - Check usage dashboard regularly at [app.tavily.com](https://app.tavily.com)
    - Set up usage alerts in Tavily (available in dashboard)
    - Implement query caching for repeated searches
    - Use appropriate search depth for your needs
    - Optimize query frequency in automated workflows
    - Review request patterns monthly

    **Usage optimization tips:**
    - Basic search depth uses fewer resources than advanced
    - Batch extract operations when possible (multiple URLs in one call)
    - Use map before crawl to understand site structure
    - Cache search results for frequently accessed queries

    <Tip>
      Studio can cache search results - enable caching in settings to reduce API calls for repeated queries.
    </Tip>
  </Accordion>

  <Accordion title="Content Filtering" icon="filter">
    Configure search filters to control results quality:

    - Use domain allowlists for trusted sources (include_domains)
    - Exclude unreliable or low-quality domains (exclude_domains)
    - Filter by date for recent content (time_range, start_date, end_date)
    - Adjust search depth based on research needs
    - Use topic="news" for current events
    - Control result count to manage API usage (max_results)

    **Domain filtering examples:**
    - Research: `["arxiv.org", "scholar.google.com", "ieee.org"]`
    - News: `["nytimes.com", "reuters.com", "bloomberg.com"]`
    - Tech: `["techcrunch.com", "arstechnica.com", "theverge.com"]`

    <Note>
      Proper filtering improves result quality and reduces time spent reviewing irrelevant content.
    </Note>
  </Accordion>
</AccordionGroup>

## Example Workflows

<Tabs>
  <Tab title="Market Research">
    **Scenario:** Research competitors and market trends in electric vehicle industry

    **Prompt:**
    ```text
    "Search for recent developments in the electric vehicle market, focusing on Tesla's main competitors and market share trends from the last 3 months"
    ```

    **What happens:**
    - Searches authoritative business and tech news sources
    - Filters for content from last 3 months
    - Extracts key facts, statistics, and quotes
    - Provides source citations with publication dates
    - Ranks results by relevance and authority
    - Identifies trends and patterns across sources

    **Time:** 3-5 seconds (advanced search)
    **API calls:** 1 request

    **Example Response:**
    - Market share data from industry reports
    - Recent news about BYD, Rivian, Lucid competitors
    - Analyst predictions and expert opinions
    - Statistical trends with time series data
    - Cited sources from Bloomberg, Reuters, industry publications

    **Follow-up prompts:**
    - "What are the main challenges facing EV manufacturers according to these sources?"
    - "Extract detailed content from the top 3 most relevant articles"
    - "Search for investor sentiment about EV stocks in the same time period"

    <Tip>
      Use advanced search depth for thorough research with more authoritative sources. Combine with domain filtering to focus on business publications.
    </Tip>
  </Tab>

  <Tab title="Fact Checking">
    **Scenario:** Verify claims and statements with authoritative sources

    **Prompt:**
    ```text
    "Verify this claim: 'OpenAI released GPT-5 in November 2024' - search for official announcements, press releases, and authoritative tech news"
    ```

    **What happens:**
    - Searches official OpenAI sources and major tech news
    - Prioritizes recent content within specified timeframe
    - Looks for press releases and official announcements
    - Checks multiple authoritative sources for verification
    - Provides publication dates for temporal verification
    - Returns confidence indicators through source quality

    **Time:** 2-3 seconds (basic search)
    **API calls:** 1 request

    **Example Response:**
    - Verification status based on source evidence
    - Official OpenAI blog posts or press releases
    - Major tech news coverage from TechCrunch, The Verge, etc.
    - Timeline of actual announcements
    - Context about what was actually released
    - Contradictory evidence if claim is false

    **Follow-up prompts:**
    - "What did OpenAI actually announce in November 2024?"
    - "Extract the full content from the official OpenAI announcement"
    - "Search for expert analysis of the actual release"

    <Warning>
      Always cross-reference critical information with multiple authoritative sources. Single-source verification may not be sufficient for important decisions.
    </Warning>
  </Tab>

  <Tab title="News Monitoring">
    **Scenario:** Track breaking news and monitor ongoing stories

    **Prompt:**
    ```text
    "What are the top news stories about climate change policy from the last 48 hours?"
    ```

    **What happens:**
    - Filters for very recent content (last 2 days)
    - Prioritizes news sources and wire services
    - Ranks by recency and relevance
    - Provides article summaries and headlines
    - Links to original stories with timestamps
    - Highlights key developments and quotes

    **Time:** 2-4 seconds (basic or advanced)
    **API calls:** 1 request

    **Example Response:**
    - Chronological list of recent news items
    - Headlines and brief summaries
    - Publication dates and news sources
    - Key quotes from officials or experts
    - Links to full articles
    - Geographic context (where events occurred)

    **Follow-up prompts:**
    - "Extract the full text of the top 3 stories"
    - "Search for expert analysis of these policy changes"
    - "What are the main debates mentioned across these articles?"

    **Ongoing monitoring strategy:**
    - Query every 4-6 hours for breaking news
    - Use time_range="day" for 24-hour rolling window
    - Set up domain filters for trusted news sources
    - Track specific topics with refined queries

    <Tip>
      For ongoing monitoring, query periodically throughout the day. Use topic="news" and time_range="day" for fresh results.
    </Tip>
  </Tab>

  <Tab title="Academic Research">
    **Scenario:** Find scholarly sources and peer-reviewed research

    **Prompt:**
    ```text
    "Search for recent peer-reviewed research on the effectiveness of mRNA vaccines, focusing on studies from 2023-2024"
    ```

    **What happens:**
    - Filters for academic and scholarly sources
    - Prioritizes peer-reviewed journals and publications
    - Date-range filtering for recent research
    - Extracts abstracts and key findings
    - Provides DOIs, citations, and publication info
    - Identifies methodology and conclusions

    **Time:** 4-6 seconds (advanced search recommended)
    **API calls:** 1 request

    **Example Response:**
    - List of relevant peer-reviewed papers
    - Journal names and publication dates
    - Author information and affiliations
    - Abstract summaries
    - Key findings and conclusions
    - DOIs and links to full papers
    - Citation information

    **Follow-up prompts:**
    - "Extract the full abstracts from the top 5 papers"
    - "What methodologies were used across these studies?"
    - "Search for meta-analyses or systematic reviews on this topic"

    **Domain filtering for academic sources:**
    ```text
    "Search using only these academic sources: pubmed.ncbi.nlm.nih.gov, scholar.google.com, arxiv.org, nature.com, science.org"
    ```

    <Note>
      Use domain filtering to focus on trusted academic databases and journals. Advanced search depth provides more thorough coverage of scholarly sources.
    </Note>
  </Tab>

  <Tab title="Product Research">
    **Scenario:** Compare products and gather expert reviews

    **Prompt:**
    ```text
    "Compare the top 5 noise-canceling headphones for under $300 based on recent expert reviews and user ratings from 2024"
    ```

    **What happens:**
    - Searches review sites, tech blogs, and expert publications
    - Filters for recent content (2024)
    - Extracts product specifications and features
    - Compiles expert opinions and ratings
    - Aggregates user feedback and reviews
    - Identifies pros, cons, and best use cases
    - Provides price comparisons from sources

    **Time:** 3-5 seconds (basic or advanced)
    **API calls:** 1-2 requests

    **Example Response:**
    - Product comparison with key specs
    - Expert review summaries and ratings
    - Price points from various sources
    - Key features and differentiators
    - Pros and cons for each model
    - Best-for-use-case recommendations
    - Links to full reviews

    **Follow-up prompts:**
    - "Which has the best battery life according to reviews?"
    - "Extract detailed specifications from the manufacturer websites"
    - "Search for common complaints or issues with the top-rated model"
    - "Find the best current prices for the top 3 options"

    <Tip>
      For product research, use basic search for quick comparisons or advanced for detailed technical analysis. Extract manufacturer pages for official specs.
    </Tip>
  </Tab>

  <Tab title="Competitive Intelligence">
    **Scenario:** Monitor competitor activities and strategic moves

    **Prompt:**
    ```text
    "Find any news about new product launches, funding rounds, or strategic partnerships from competitors in the AI coding assistant space from the last 30 days"
    ```

    **What happens:**
    - Searches tech news, press releases, and business publications
    - Filters for recent announcements (last month)
    - Identifies competitor companies mentioned
    - Tracks new product launches and features
    - Monitors funding and investment news
    - Highlights strategic partnerships and acquisitions
    - Provides timeline of events

    **Time:** 3-5 seconds (advanced recommended)
    **API calls:** 1-2 requests

    **Example Response:**
    - Recent competitor announcements
    - Product launch details and features
    - Funding rounds with amounts and investors
    - Partnership announcements
    - Strategic direction indicators
    - Market positioning insights
    - Analyst commentary and expert opinions

    **Follow-up prompts:**
    - "Extract full press releases from the top 3 announcements"
    - "Search for technical details about the new features launched"
    - "What are analysts saying about these strategic moves?"
    - "Compare feature sets between our product and theirs"

    **Regular monitoring workflow:**
    1. Weekly competitive search (7-day time range)
    2. Extract key announcements for detailed analysis
    3. Track emerging trends across multiple competitors
    4. Monitor sentiment in industry commentary

    <Info>
      Regular competitive monitoring helps you stay ahead of market trends. Schedule weekly searches with time_range="week" for rolling updates.
    </Info>
  </Tab>

  <Tab title="Content Research">
    **Scenario:** Gather information and sources for writing projects

    **Prompt:**
    ```text
    "I'm writing an article about remote work trends in 2024. Find the latest statistics, expert opinions, company case studies, and research reports"
    ```

    **What happens:**
    - Searches for relevant statistics and data
    - Finds expert interviews, quotes, and commentary
    - Locates company case studies and examples
    - Gathers supporting research and reports
    - Provides diverse perspectives (pro/con, different industries)
    - Cites all sources properly with URLs
    - Identifies authoritative voices in the field

    **Time:** 4-6 seconds (advanced search for comprehensive results)
    **API calls:** 2-3 requests (search + extract key sources)

    **Example Response:**
    - Key statistics with authoritative sources
    - Expert quotes with attribution
    - Company case studies (Buffer, GitLab, etc.)
    - Research report summaries
    - Trend analysis from multiple sources
    - Supporting data and evidence
    - Diverse industry perspectives

    **Structured research workflow:**
    1. **Initial search**: Broad topic research
    2. **Extract sources**: Pull full content from key articles
    3. **Specific queries**: Deep dive into sub-topics
    4. **Fact verification**: Cross-reference claims
    5. **Quote mining**: Find expert opinions

    **Follow-up prompts:**
    - "Extract the full methodology from this research report: [url]"
    - "Find specific statistics about remote work productivity"
    - "Search for contrarian viewpoints about remote work effectiveness"
    - "What are the latest tools mentioned for remote collaboration?"

    <Tip>
      Use specific queries for each section of your content. Start broad for overview, then drill into specific data, quotes, and examples you need.
    </Tip>
  </Tab>

  <Tab title="Technical Research">
    **Scenario:** Find technical documentation and troubleshoot issues

    **Prompt:**
    ```text
    "Search for solutions to 'Python asyncio event loop already running' error in Jupyter notebooks with working code examples"
    ```

    **What happens:**
    - Searches Stack Overflow and tech forums
    - Finds GitHub issues and discussions
    - Locates official documentation
    - Identifies common solutions and patterns
    - Provides code examples and snippets
    - Explains root causes and context
    - Ranks by solution effectiveness

    **Time:** 2-4 seconds (basic search often sufficient)
    **API calls:** 1-2 requests

    **Example Response:**
    - Multiple solution approaches
    - Code snippets with explanations
    - Root cause analysis
    - Links to Stack Overflow discussions
    - GitHub issue threads
    - Official documentation references
    - Community-vetted solutions

    **Follow-up prompts:**
    - "Extract the full solution from the top Stack Overflow answer"
    - "Find the official Python documentation about this issue"
    - "Search for alternatives to asyncio for this use case"
    - "What are the best practices to avoid this error?"

    **Technical documentation workflow:**
    1. Search for specific error or concept
    2. Extract relevant documentation sections
    3. Crawl official docs for related topics
    4. Search for real-world examples and patterns

    <Warning>
      Always test solutions in a safe environment before applying to production. Verify code examples are from trusted sources.
    </Warning>
  </Tab>

  <Tab title="Website Analysis">
    **Scenario:** Analyze website structure and systematically extract content

    **Prompt:**
    ```text
    "Map the documentation structure of docs.react.dev, then crawl all pages about hooks and extract their content"
    ```

    **What happens:**
    1. **Mapping phase**: Creates site structure map
    2. **Analysis**: Identifies relevant sections
    3. **Targeted crawl**: Extracts content from hooks pages
    4. **Content processing**: Formats extracted information

    **Time:** 5-10 seconds (map: 2-3 sec, crawl: 3-7 sec)
    **API calls:** 2 requests (1 map + 1 crawl)

    **Example Response from map:**
    ```
    Site Map Results:
    Base URL: https://docs.react.dev

    Mapped Pages:
    [1] https://docs.react.dev/reference/react
    [2] https://docs.react.dev/reference/react/hooks
    [3] https://docs.react.dev/reference/react/useState
    [4] https://docs.react.dev/reference/react/useEffect
    [5] https://docs.react.dev/reference/react/useContext
    ... (more URLs)
    ```

    **Example Response from crawl:**
    ```
    Crawl Results:
    Base URL: https://docs.react.dev

    Crawled Pages:
    [1] https://docs.react.dev/reference/react/useState
    Content: useState is a React Hook that lets you add state...

    [2] https://docs.react.dev/reference/react/useEffect
    Content: useEffect is a React Hook that lets you synchronize...
    ```

    **Advanced usage:**
    - Use select_paths to filter: `["/reference/react/use.*"]`
    - Control depth and breadth for focused crawling
    - Use instructions: "Only return pages about React hooks"
    - Extract in markdown format for better code block formatting

    <Tip>
      Always map before crawling large sites to understand structure. Use path patterns to focus on specific sections and reduce API usage.
    </Tip>
  </Tab>
</Tabs>

## Troubleshooting

<AccordionGroup>
  <Accordion title="Rate Limit Exceeded" icon="triangle-exclamation">
    **Error Message:**
    ```
    429 Too Many Requests - Monthly quota exceeded
    ```

    **Cause:** You've exceeded your monthly request limit (1,000 for Free tier)

    **Solutions:**
    - Check your usage at [app.tavily.com](https://app.tavily.com) dashboard
    - Wait until next month for automatic limit reset (1st of the month)
    - Upgrade to a higher tier plan ($49/mo for 10,000 requests)
    - Implement result caching for repeated queries
    - Optimize query frequency in workflows
    - Use more specific queries to reduce trial-and-error searches
    - Consider batching research tasks monthly

    **Usage optimization:**
    - Use basic search depth when advanced isn't needed
    - Cache search results in your workflow
    - Batch multiple URLs in single extract call
    - Map before crawling to plan efficiently

    <Tip>
      Enable caching in Studio settings to automatically reuse recent search results and reduce API calls.
    </Tip>
  </Accordion>

  <Accordion title="Invalid API Key" icon="key">
    **Error Message:**
    ```
    401 Unauthorized - Invalid API key
    ```

    **Solutions:**
    - Verify your API key in Studio server settings (check for extra spaces)
    - Confirm your Tavily account is active
    - Check if key was revoked or regenerated
    - Regenerate key from [Tavily dashboard](https://app.tavily.com) if necessary
    - Ensure you copied the entire key without truncation
    - Verify no special characters were added during copy/paste

    **To update API key in Studio:**
    1. Navigate to Connections ‚Üí Tavily
    2. Click Edit Configuration
    3. Paste new API key (no quotes, no spaces)
    4. Save changes
    5. Verify "Active" status appears

    <Warning>
      API keys are account-specific. Don't share keys between team members - each person should have their own account and key.
    </Warning>
  </Accordion>

  <Accordion title="No Results Found" icon="circle-question">
    **Error Message:**
    ```
    No relevant results found for query
    ```

    **Solutions:**
    - Broaden your search query (use less specific terms)
    - Remove overly restrictive domain filters
    - Expand time range or remove date filters
    - Check spelling and terminology
    - Try alternative keywords or phrases
    - Use more general terms first, then narrow with follow-up queries
    - Verify the topic has publicly available web content
    - Try removing country restrictions if set

    **Query optimization examples:**
    - ‚ùå Too specific: "John Smith from Acme Corp's opinion on XYZ from last Tuesday"
    - ‚úÖ Better: "Expert opinions on XYZ technology"
    - ‚úÖ Best: "Recent analysis of XYZ technology trends"

    **Troubleshooting steps:**
    1. Try basic query without filters
    2. If results appear, gradually add filters back
    3. Test with known queries that should have results
    4. Verify internet connectivity

    <Tip>
      Start with broad queries to establish that content exists, then use follow-up queries to narrow focus based on initial results.
    </Tip>
  </Accordion>

  <Accordion title="Slow Response Time" icon="clock">
    **Issue:** Searches taking longer than expected

    **Solutions:**
    - Check your internet connection speed
    - Verify Tavily API status at [status.tavily.com](https://status.tavily.com) (if available)
    - Reduce search depth to "basic" for faster results
    - Decrease max_results parameter (e.g., 5 instead of 20)
    - Simplify complex multi-part queries
    - Break compound questions into separate queries
    - Consider query complexity (more filters = more processing)
    - Check for Tavily service announcements

    **Typical response times:**
    - Basic search: 2-3 seconds
    - Advanced search: 4-6 seconds
    - Extract (single URL): 1-2 seconds
    - Extract (multiple URLs): 2-4 seconds
    - Crawl (depth 1, 10 pages): 3-5 seconds
    - Crawl (depth 2, 50 pages): 8-12 seconds
    - Map (50 URLs): 2-4 seconds

    **Performance tips:**
    - Use basic search for quick lookups
    - Advanced search for comprehensive research only
    - Limit crawl depth and breadth for faster results
    - Map sites before crawling to understand scope

    <Note>
      Advanced searches and deep crawls are more thorough but take longer. Balance speed vs. comprehensiveness based on your needs.
    </Note>
  </Accordion>

  <Accordion title="Poor Result Quality" icon="thumbs-down">
    **Issue:** Results are not relevant or low quality

    **Solutions:**
    - Use more specific, detailed search queries
    - Add domain filters for trusted, authoritative sources
    - Exclude known low-quality or unreliable domains
    - Increase search depth from "basic" to "advanced"
    - Provide more context in your query
    - Use quotes for exact phrase matching
    - Filter by date for recent, current content
    - Specify topic="news" for current events

    **Query improvement examples:**

    **Basic:** "AI news"
    - Too broad, mixed quality results

    **Better:** "Latest AI breakthroughs in healthcare 2024"
    - More specific topic, time frame, industry

    **Best:** "Recent FDA-approved AI medical diagnostic tools and clinical trial results"
    - Very specific, implies authoritative sources

    **Domain filtering for quality:**
    - Academic: `["arxiv.org", "scholar.google.com", "pubmed.gov"]`
    - Business: `["bloomberg.com", "reuters.com", "wsj.com"]`
    - Tech: `["arstechnica.com", "techcrunch.com", "theverge.com"]`

    <Tip>
      More specific queries with context and constraints produce higher quality, more relevant results. Invest time in query crafting.
    </Tip>
  </Accordion>

  <Accordion title="Domain Filtering Not Working" icon="filter">
    **Issue:** Domain filters not producing expected results

    **Solutions:**
    - Verify domain format: use "example.com" not "https://example.com"
    - Check for typos in domain names
    - Ensure domains actually have content matching your query
    - Don't over-filter (too many restrictions = no results)
    - Use include OR exclude, not both for same domains
    - Test without filters first to verify content exists
    - Check that domain is spelled exactly as it appears in URLs

    **Correct domain format examples:**

    ‚úÖ Correct:
    ```json
    ["nytimes.com", "washingtonpost.com", "reuters.com"]
    ```

    ‚ùå Incorrect:
    ```json
    ["https://nytimes.com", "www.washingtonpost.com", "https://www.reuters.com/"]
    ```

    **Testing approach:**
    1. Search without domain filters first
    2. Verify which domains appear in results
    3. Use exact domain names from those results
    4. Add filters one at a time

    <Warning>
      Overly restrictive domain filters can result in zero results even for valid queries. Start with 2-3 trusted domains, not 20.
    </Warning>
  </Accordion>

  <Accordion title="Crawl/Map Incomplete Results" icon="spider">
    **Issue:** Crawler or mapper returning fewer pages than expected

    **Solutions:**
    - Check site's robots.txt (some sites block crawlers)
    - Increase limit parameter (default is 50)
    - Increase max_depth to explore deeper
    - Increase max_breadth to follow more links per page
    - Remove overly restrictive select_paths patterns
    - Verify select_domains regex is correct
    - Check if site requires authentication (crawler can't access)
    - Some sites may have JavaScript-only navigation (not crawlable)

    **Understanding limits:**
    - `limit`: Total pages to process before stopping
    - `max_depth`: How many levels deep from base URL
    - `max_breadth`: How many links per page

    **Example:**
    - depth=2, breadth=10, limit=50
    - Level 0: 1 page (base URL)
    - Level 1: up to 10 pages (first 10 links from base)
    - Level 2: up to 100 pages (10 links from each L1 page)
    - But limited to 50 total pages by `limit` parameter

    **Optimization:**
    - Map first to understand actual site structure
    - Use path patterns to focus on relevant sections
    - Adjust depth/breadth based on map results
    - Increase limit gradually to avoid over-crawling

    <Tip>
      Always map a site first to understand its structure and size before crawling. This helps you set appropriate limits.
    </Tip>
  </Accordion>

  <Accordion title="Server Connection Issues" icon="server">
    **Error Message:**
    ```
    Server connection timeout or unavailable
    ```

    **Solutions:**
    - Check your internet connection
    - Verify server is enabled in Studio (Connections)
    - Try disabling and re-enabling the Tavily server
    - Check Tavily API status (look for status page or announcements)
    - Verify Studio is not experiencing service interruptions
    - Clear Studio cache and retry
    - Try a simple test query to isolate issue
    - Contact support if issue persists

    **Connection troubleshooting steps:**
    1. Test internet connection (open a website)
    2. Check Studio status (other servers working?)
    3. Verify API key is valid (test in Tavily dashboard)
    4. Try disabling/re-enabling in Studio
    5. Check for error details in Studio logs (if available)

    <Info>
      Studio manages all server infrastructure automatically - no local setup or maintenance required.
    </Info>
  </Accordion>

  <Accordion title="Tools Not Triggering" icon="tools">
    **Issue:** Studio doesn't use Tavily tools when expected

    **Solutions:**
    - Be more explicit: mention "search the web" or "use Tavily"
    - Verify server shows "Active" status in Studio
    - Check API key is correctly configured
    - Provide clear action verbs: "search", "find", "extract", "crawl"
    - Include specific URLs when you want extraction
    - Don't ask Studio to find URLs - provide them directly
    - Use phrases that indicate web search intent

    **Example effective prompts:**

    ‚úÖ Good - Clear web search intent:
    - "Search the web for recent AI developments"
    - "Find the latest news about climate policy"
    - "Search for expert opinions on remote work"

    ‚úÖ Good - Explicit extraction:
    - "Extract content from this URL: [url]"
    - "Crawl docs.example.com and extract all content"

    ‚ùå Ambiguous - May not trigger tools:
    - "What's the latest on AI?" (might use general knowledge)
    - "Tell me about climate policy" (might not search web)
    - "What does this page say?" (no URL provided)

    <Tip>
      Make your intent clear: use "search the web", "extract from URL", "crawl this site" to explicitly signal tool usage.
    </Tip>
  </Accordion>
</AccordionGroup>

## Links & Resources

<CardGroup cols={2}>
  <Card title="GitHub Repository" icon="github" href="https://github.com/tavily-ai/tavily-mcp">
    View source code, report issues, and contribute
  </Card>

  <Card title="Tavily Documentation" icon="book" href="https://docs.tavily.com">
    Official Tavily API reference and guides
  </Card>

  <Card title="Tavily Dashboard" icon="gauge" href="https://app.tavily.com">
    Manage your API keys and monitor usage
  </Card>

  <Card title="Report Issues" icon="bug" href="https://github.com/tavily-ai/tavily-mcp/issues">
    Found a bug? Submit an issue on GitHub
  </Card>
</CardGroup>

## Learning Resources

<AccordionGroup>
  <Accordion title="Search Optimization" icon="magnifying-glass-plus">
    **Tips for Better Search Results:**

    **1. Query Crafting:**
    - Be specific: Include context, timeframe, and domain
    - Use industry terminology and proper nouns
    - Specify what you're looking for (research, news, opinions, statistics)
    - Include qualifiers (recent, official, expert, peer-reviewed)

    **2. Search Depth Selection:**
    - **Basic**: Quick lookups, general information, common topics
    - **Advanced**: Comprehensive research, academic work, competitive intelligence

    **3. Effective Filtering:**
    - Domain allowlists for trusted sources
    - Time ranges for current information
    - Topic selection (general vs. news)
    - Geographic filtering when relevant

    **4. Iterative Research:**
    - Start broad to understand landscape
    - Refine with follow-up questions
    - Extract full content from promising sources
    - Cross-reference across multiple results

    **5. Source Evaluation:**
    - Check publication dates
    - Verify author credentials
    - Look for citations and references
    - Compare across multiple sources
    - Prioritize primary sources over secondary

    <Tip>
      Think of Tavily as a research assistant - the more context and direction you provide, the better the results.
    </Tip>
  </Accordion>

  <Accordion title="Research Workflows" icon="list-check">
    **Effective Research Process:**

    **Phase 1: Discovery (Basic Search)**
    - Start with broad queries to map the topic
    - Identify key terms, names, and concepts
    - Find authoritative sources and publications
    - Understand the current state of knowledge

    **Phase 2: Deep Dive (Advanced Search + Extract)**
    - Use advanced search for comprehensive coverage
    - Apply domain filters for quality sources
    - Extract full content from key articles
    - Identify gaps and questions for further research

    **Phase 3: Verification (Cross-referencing)**
    - Verify facts across multiple sources
    - Check publication dates and recency
    - Identify consensus vs. outlier opinions
    - Note conflicting information for further investigation

    **Phase 4: Specialized Research (Crawl/Map)**
    - Map site structures for systematic coverage
    - Crawl documentation or knowledge bases
    - Extract structured information at scale
    - Build comprehensive topic databases

    **Phase 5: Synthesis**
    - Combine findings into coherent insights
    - Track all sources and citations
    - Identify trends and patterns
    - Formulate conclusions based on evidence

    <Note>
      Studio automatically tracks sources and citations throughout your research session for easy reference.
    </Note>
  </Accordion>

  <Accordion title="Source Evaluation" icon="check-double">
    **Assessing Source Quality and Reliability:**

    **Authority Indicators:**
    - Author credentials and expertise
    - Publication reputation and peer review status
    - Citations and references provided
    - Institutional backing or sponsorship
    - Domain authority (.edu, .gov, established publications)

    **Recency Evaluation:**
    - Publication date matches your needs
    - Updates and corrections noted
    - Reflects current understanding
    - Historical context provided when needed

    **Objectivity Assessment:**
    - Balanced presentation of evidence
    - Multiple perspectives included
    - Clear distinction between fact and opinion
    - Potential biases disclosed
    - Funding sources transparent

    **Evidence Quality:**
    - Primary sources cited
    - Data and statistics provided
    - Methodology clearly described
    - Reproducible or verifiable claims
    - Expert consensus acknowledged

    **Cross-referencing:**
    - Compare across multiple sources
    - Look for consensus on key facts
    - Note areas of disagreement
    - Identify potential errors or outliers
    - Verify claims with official sources

    **Red Flags:**
    - No author or source attribution
    - Sensational headlines
    - No citations or sources
    - Conflicts with established facts
    - Poor grammar or unprofessional presentation

    <Warning>
      AI summaries are helpful for efficiency but always verify critical information with original authoritative sources.
    </Warning>
  </Accordion>

  <Accordion title="Advanced Query Techniques" icon="brain">
    **Strategies for Complex Research:**

    **1. Comparison Queries:**
    ```text
    "Compare [A] vs [B] based on [criteria] with recent data"
    "What are the main differences between [A] and [B]?"
    "Pros and cons of [topic] from expert perspectives"
    ```

    **2. Trend Analysis:**
    ```text
    "What are emerging trends in [industry] for 2024?"
    "How has [topic] evolved over the past [timeframe]?"
    "Latest developments in [field] according to recent research"
    ```

    **3. Expert Insights:**
    ```text
    "What do leading experts say about [topic]?"
    "Find recent expert opinions and analysis on [issue]"
    "Search for thought leadership content about [subject]"
    ```

    **4. Evidence Gathering:**
    ```text
    "Find statistics and data about [topic] from authoritative sources"
    "What research and studies support [claim]?"
    "Search for peer-reviewed evidence on [medical/scientific topic]"
    ```

    **5. Temporal Queries:**
    ```text
    "Latest news from the past 24 hours about [event]"
    "Historical analysis of [topic] from 2020 to present"
    "Track how [narrative/understanding] has changed over time"
    ```

    **6. Geographic Specificity:**
    ```text
    "US market analysis of [product/service]"
    "European regulations regarding [topic]"
    "Search prioritizing sources from [country]"
    ```

    **7. Format-Specific Searches:**
    ```text
    "Find official documentation for [technical topic]"
    "Search for case studies about [business challenge]"
    "Locate white papers on [industry topic]"
    "Find tutorial or how-to content for [skill]"
    ```

    <Tip>
      Frame queries as specific research objectives rather than vague questions. Include context, constraints, and desired source types for best results.
    </Tip>
  </Accordion>

  <Accordion title="Crawling & Mapping Strategies" icon="spider">
    **Effective Website Analysis:**

    **When to Use Map vs. Crawl:**

    **Use Map when:**
    - Understanding site structure first
    - You need a site overview
    - Planning a targeted crawl
    - Checking link validity
    - Auditing site architecture
    - You don't need page content yet

    **Use Crawl when:**
    - You need actual page content
    - Building a knowledge base
    - Extracting documentation
    - Collecting data at scale
    - Deep content analysis required
    - Following up after mapping

    **Parameter Strategy:**

    **max_depth:**
    - 1: Current page + direct links (homepage + main sections)
    - 2: Two levels deep (homepage ‚Üí section ‚Üí subsection)
    - 3+: Deep exploration (use cautiously with large sites)

    **max_breadth:**
    - 10: Focused exploration, main links only
    - 20: Balanced coverage (default)
    - 50+: Comprehensive but slower

    **limit:**
    - 20: Quick sampling
    - 50: Medium site coverage (default)
    - 100: Large site exploration
    - 200+: Comprehensive archival

    **Path Patterns:**
    ```javascript
    ["/docs/.*"]           // Only documentation pages
    ["/api/v2/.*"]        // Specific API version
    ["/blog/\\d{4}/.*"]   // Blog posts with year
    ["/products/.*"]       // Product pages
    ```

    **Domain Patterns:**
    ```javascript
    ["^docs\\.example\\.com$"]           // Exact subdomain
    ["^.*\\.example\\.com$"]             // All subdomains
    ["^(docs|api)\\.example\\.com$"]     // Specific subdomains
    ```

    **Best Practices:**
    1. Always map large sites first
    2. Use path patterns to focus on relevant sections
    3. Start with conservative limits
    4. Increase depth/breadth based on map results
    5. Use instructions for semantic filtering
    6. Respect site's robots.txt and crawling policies

    <Warning>
      Large sites with high depth/breadth can consume many pages quickly. Start conservatively and adjust based on results.
    </Warning>
  </Accordion>

  <Accordion title="API Usage & Costs" icon="coins">
    **Understanding Request Consumption:**

    **Request Counting:**
    - **Search**: 1 request per query (regardless of max_results)
    - **Extract**: 1 request per call (multiple URLs = still 1 request)
    - **Crawl**: 1 request per session (all pages crawled = 1 request)
    - **Map**: 1 request per session (all URLs mapped = 1 request)

    **Cost-Effective Strategies:**

    **1. Batch Operations:**
    ```text
    ‚úÖ Extract multiple URLs in one call:
    "Extract content from these 5 URLs: [url1, url2, url3, url4, url5]"
    ‚Üí 1 request

    ‚ùå Extract URLs separately:
    5 separate extract calls
    ‚Üí 5 requests
    ```

    **2. Strategic Search Depth:**
    - Use basic search (faster, same request count)
    - Reserve advanced for critical research only
    - Both consume 1 request, but advanced may take longer

    **3. Efficient Crawling:**
    ```text
    ‚úÖ Map first, then targeted crawl:
    Map (1 request) ‚Üí understand structure ‚Üí focused crawl (1 request)
    ‚Üí 2 requests total, but more efficient

    ‚ùå Blind crawling:
    Large crawl with poor parameters ‚Üí wasted pages
    ‚Üí 1 request, but inefficient use
    ```

    **4. Query Optimization:**
    - Craft specific queries to get right results first time
    - Poor query ‚Üí no results ‚Üí retry ‚Üí multiple requests
    - Good query ‚Üí useful results ‚Üí one request

    **5. Result Caching:**
    - Enable caching in Studio for repeated queries
    - Cache research results in your workflow
    - Avoid re-searching the same topics

    **6. Time-Based Planning:**
    - Batch research tasks monthly if possible
    - Schedule regular monitoring (weekly, not daily)
    - Consolidate similar queries into comprehensive searches

    **Monthly Planning (1,000 requests):**
    - Daily research: ~33 requests/day
    - Weekly monitoring: ~250 requests/week
    - Mix of search (80%), extract (15%), crawl (5%)

    <Tip>
      Track your usage patterns in the Tavily dashboard. Adjust research habits based on monthly consumption trends.
    </Tip>
  </Accordion>
</AccordionGroup>

---

**Ready to Research?** Enable Tavily in NimbleBrain Studio and start with the free tier. You get 1,000 requests per month to explore all 4 powerful research tools!
